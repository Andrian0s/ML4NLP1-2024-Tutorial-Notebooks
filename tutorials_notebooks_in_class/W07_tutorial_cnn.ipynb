{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2023-Tutorial-Notebooks/blob/main/tutorial_notebooks/07_tutorial_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7Ic2n5f6Wve"
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyrhwMu0BoJZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TITzFYHoOKeT"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data, torchvision as tv\n",
        "import lightning as L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNNEe9D1CvNW"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I3RZErWOKeU",
        "outputId": "6cefa911-18f2-4967-9056-66a332990abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 7 GPU(s) available.\n",
            "Device name: NVIDIA GeForce GTX TITAN X\n"
          ]
        }
      ],
      "source": [
        "# use the GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY5WvZMPBuMZ"
      },
      "outputs": [],
      "source": [
        "# download the dataset with wget\n",
        "# if the dataset is on github, try git clone instead.\n",
        "!wget -P \"data/\" https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
        "# unpack the file\n",
        "!tar xvzf 'data/rt-polaritydata.tar.gz' -C 'data/'\n",
        "!mv data/rt-polaritydata/rt-polarity.neg data/\n",
        "!mv data/rt-polaritydata/rt-polarity.pos data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZV7yzsYDk5D"
      },
      "outputs": [],
      "source": [
        "# import the dataset (txt file) line by line\n",
        "def load_text(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        texts = []\n",
        "        for line in f:\n",
        "            texts.append(line.decode(errors='ignore').lower().strip())\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxRgXT3vD2ix"
      },
      "outputs": [],
      "source": [
        "neg_text = load_text(\"movie_review_data/data/rt-polarity.neg\")\n",
        "pos_text = load_text(\"movie_review_data/data/rt-polarity.pos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI5fdo22UKqe"
      },
      "outputs": [],
      "source": [
        "# concat negative and positive texts\n",
        "texts = neg_text + pos_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64j9QLGHOKeW",
        "outputId": "bb57b346-392f-4657-daa2-ae0fbf114a44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['simplistic , silly and tedious .',\n",
              " \"it's so laddish and juvenile , only teenage boys could possibly find it funny .\",\n",
              " 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .',\n",
              " '[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation .',\n",
              " 'a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6tsNU7gUgoF"
      },
      "outputs": [],
      "source": [
        "# we know the order in texts variable, so we can label it accordingly\n",
        "labels = np.array([0]*len(neg_text) + [1]*len(pos_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqP3wWAbffH_"
      },
      "outputs": [],
      "source": [
        "def tokenize(texts):\n",
        "  \"\"\"\n",
        "  Assign unique id to each token\n",
        "  \"\"\"\n",
        "  max_len = 0\n",
        "  tokenized_texts = []\n",
        "  word2idx = {}\n",
        "\n",
        "  # Add <pad> and <unk> tokens to the vocabulary\n",
        "  word2idx['<pad>'] = 0\n",
        "  word2idx['<unk>'] = 1\n",
        "\n",
        "  # Building our vocab from the corpus starting from index 2\n",
        "  idx = 2\n",
        "  for sent in texts:\n",
        "    tokenized_sent = nlp(sent)\n",
        "    # Add `tokenized_sent` to `tokenized_texts`\n",
        "    tokenized_texts.append(tokenized_sent)\n",
        "    # Add new token to `word2idx`\n",
        "    for token in tokenized_sent:\n",
        "      # string any token objects are different things, be careful.\n",
        "      if token.text not in word2idx:\n",
        "        word2idx[token.text] = idx\n",
        "        idx += 1\n",
        "\n",
        "        # Update `max_len`\n",
        "    max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "  return tokenized_texts, word2idx, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N0qAlGJicxG"
      },
      "outputs": [],
      "source": [
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "        # Pad sentences to max_len\n",
        "        tokenized_padded_sent = list(tokenized_sent) + ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        # Encode tokens to input_ids\n",
        "        input_id = [word2idx.get(str(token)) for token in tokenized_padded_sent]\n",
        "        input_ids.append(input_id)\n",
        "\n",
        "    return np.array(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0GvkEDscK41"
      },
      "outputs": [],
      "source": [
        "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqwxTRS-rzEg"
      },
      "outputs": [],
      "source": [
        "# Convert data type to torch.Tensor\n",
        "train_inputs = torch.from_numpy(input_ids)\n",
        "labels = torch.from_numpy(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GbJuyVPg9Mg",
        "outputId": "8edde1f0-bec1-4985-e634-0d165cbe40a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['simplistic , silly and tedious .',\n",
              " \"it's so laddish and juvenile , only teenage boys could possibly find it funny .\",\n",
              " 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVBeYnD8OKeX",
        "outputId": "1eea7325-2fba-4e56-8c5a-47d05ba264dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 2,  3,  4,  5,  6,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 8,  9, 10, 11,  5, 12,  3, 13, 14, 15, 16, 17, 18,  8, 19,  7,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "        [20,  5, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 23,\n",
              "         24, 36, 37,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_inputs[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpfREW0xbNyc"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "# Create DataLoader for training data\n",
        "all_data = TensorDataset(train_inputs, labels)\n",
        "dataset = TensorDataset(train_inputs, labels)\n",
        "total_len = len(dataset)\n",
        "train_size = int(0.7 * total_len)\n",
        "val_size = int(0.2 * total_len)\n",
        "test_size = total_len - train_size - val_size\n",
        "\n",
        "train_data, val_data, test_data = data.random_split(dataset, [train_size, val_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBxk2NHEOKeY"
      },
      "outputs": [],
      "source": [
        "# Specify batch_size\n",
        "batch_size = 8\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data)\n",
        "test_dataloader = DataLoader(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIzU9ASmO7Of"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size=len(word2idx),\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[3, 4, 5],\n",
        "                 num_filters=[100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "        The constructor for CNN class.\n",
        "        Args:\n",
        "            vocab_size (int): Need to be specified when pretrained word\n",
        "                embeddings are not used.\n",
        "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
        "            num_filters (List[int]): List of number of filters, has the same\n",
        "                length as `filter_sizes`. Default: [100, 100, 100]\n",
        "            n_classes (int): Number of classes. Default: 2\n",
        "            dropout (float): Dropout rate. Default: 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx=0,\n",
        "                                          max_norm=5.0)\n",
        "        # Conv Network\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"Perform a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
        "                (batch_size, max_sent_length)\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
        "                n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings from `input_ids`. Output shape: (batch_size, max_len, embed_dim)\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        # Output shape: (b, embed_dim, max_len)\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "\n",
        "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list]\n",
        "\n",
        "        # Concatenate x_pool_list to feed the fully connected layer.\n",
        "        # Output shape: (b, sum(num_filters))\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
        "                         dim=1)\n",
        "        print(x_fc.shape)\n",
        "\n",
        "        # Compute logits. Output shape: (b, n_classes)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASIIrImWVhEC"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate CNN model\n",
        "model = CNN(embed_dim=300,\n",
        "            filter_sizes=[3, 4, 5],\n",
        "            num_filters=[100, 100, 100],\n",
        "            num_classes=2,\n",
        "            dropout=0.5)\n",
        "\n",
        "# Send model to `device` (GPU/CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Instantiate Adadelta optimizer\n",
        "optimizer = optim.Adadelta(model.parameters(),\n",
        "                               lr=0.01,\n",
        "                               rho=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1asuMytjXK8c",
        "outputId": "4d0bc002-0f6f-42c5-9b1a-26f34352a228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss \n",
            "------------------------------------------------------------\n",
            "torch.Size([8, 300])\n",
            "   1    |   0.693357  \n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Start training loop\n",
        "print(\"Start training...\\n\")\n",
        "print(f\"{'Epoch':^7} | {'Train Loss':^12}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for epoch_i in range(1):\n",
        "  total_loss = 0\n",
        "  # Put the model into the training mode\n",
        "  model.train()\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Load batch to GPU\n",
        "    b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Zero out any previously calculated gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Perform a forward pass. This will return logits.\n",
        "    logits = model(b_input_ids)\n",
        "    break\n",
        "\n",
        "    # Compute loss and accumulate the loss values\n",
        "    loss = loss_fn(logits, b_labels)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Perform a backward pass to calculate gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate the average loss over the entire training data\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "  print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH24RFQvOKeZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}