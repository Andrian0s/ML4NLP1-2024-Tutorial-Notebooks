{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vOpydQgDX3KK"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/tutorials_notebooks_in_class/W02_Intro_to_sklearn_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gzHo80DX3J0"
      },
      "source": [
        "# Introduction to scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVB8QC6bX3J5"
      },
      "source": [
        "We would like to introduce you to scikit-learn with the help of an instructional example about text classification. We will cover the most basic principles and ideas about scikit-learn in this notebook. This tutorial is inspired by the sklearn tutorial on http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html, but contains a few more explanations and is suited to introduce scikit-learn in class.\n",
        "\n",
        "$Author$: Phillip Str√∂bel\n",
        "\n",
        "With minor adjustments from: Janis Goldzycher, Andrianos Michail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuLQDR5MX3J6"
      },
      "source": [
        "## Data\n",
        "\n",
        "Get the data from http://qwone.com/~jason/20Newsgroups/. We will work with the 20news-bydate.tar.gz data set. Unzip it to a suitable destination. Here, all the data lies in the data folder. To our convenience, it has already been split into a training and a test set, so we don't have to care about this. What we need to do though is get the data and put it into a dataframe (you could also only work with dictionaries or other data containers). We do this for both the training and the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuV1TvUNX3J7"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_df(path_to_data, random_state=42):\n",
        "    \"\"\"\n",
        "    Takes the path of a folder containing all the subfolders (which contain the actual documents).\n",
        "    Builds a pandas datafram with document ids, the text and the label.\n",
        "    :param path_to_data: path to top folder as a string\n",
        "    :param random_state: integer, seed for shuffling\n",
        "    :return: pandas dataframe with all th\n",
        "    \"\"\"\n",
        "    doc_list = list()  # doc_list now: [[doc<str>, label<str>], ...]\n",
        "\n",
        "    for category in os.listdir(path_to_data):\n",
        "        for document in os.listdir(os.path.join(path_to_data, category)):\n",
        "            doc = open(os.path.join(path_to_data, category, document), 'r', encoding='latin-1').read().replace('\\n', ' ')\n",
        "            doc_list.append([doc, category])\n",
        "\n",
        "    df = pd.DataFrame(doc_list, columns=['text', 'label'])\n",
        "\n",
        "    return df.sample(frac=1, random_state=random_state) # return and shuffle dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhNP5KxyX3J8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "b83621a0-e141-4db2-f3db-f9e969c96c64"
      },
      "source": [
        "train = create_df('20news-bydate-train')\n",
        "test = create_df('20news-bydate-test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a8574dbdd284>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'20news-bydate-train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'20news-bydate-test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-7c3a3a57a08a>\u001b[0m in \u001b[0;36mcreate_df\u001b[0;34m(path_to_data, random_state)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdoc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# doc_list now: [[doc<str>, label<str>], ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20news-bydate-train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuvWeXL2X3J8"
      },
      "source": [
        "Several ways to inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSiZBH2nX3J9"
      },
      "source": [
        "print('training size: ', train.shape)\n",
        "print('test size: ', test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1hM5sIuX3J-"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "63bHHHJEX3J_"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfXDPPx2X3KA"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXZ8GGUIX3KA"
      },
      "source": [
        "train.groupby('label').size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIz2S4PX3KA"
      },
      "source": [
        "As usual, we split the labels from the training and the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r6WywPEX3KB"
      },
      "source": [
        "X_train = train.text\n",
        "y_train = train.label\n",
        "X_test = test.text\n",
        "y_test = test.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCaHvzBVX3KB"
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmuRdUSSX3KC"
      },
      "source": [
        "Series is just a \"One-dimensional ndarray with axis labels\". Let's see if we got this right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3PoWkuRX3KC"
      },
      "source": [
        "print('Training set shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test set shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJvHvu5pX3KD"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyUIWl1BX3KD"
      },
      "source": [
        "## Preprocessing\n",
        "So far, so good! But we know that machine learning algorithms cannot work with text data directly. So we need to vectorise the data somehow. also, we might do some preprocessing. Let's see how we can tackle these problems.\n",
        "### Vectorise the data\n",
        "Luckily, sklearn offers some nice classes which help us. We should tokenise the data and then vectorise it. Conveniently, sklearns `CountVectoriser()` does exactly that. Let's see how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_T0KafxX3KD"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)  # num_docs x num_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jINuDaIX3KD"
      },
      "source": [
        "Basically, the three central methods in sklearn are `transform`, `fit`, `fit_transform`, and `predict`. We will see how each of these work and when to use them. We have alredy made use of `fit_transform`. Instead of using this method, we could have called the method `fit` on the training set first and the use `transform` to vectorise the data (to 'transform' it). With the fitted `CountVectorizer` we can now transform other data, like for example the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Ma6B6DX3KE"
      },
      "source": [
        "X_test_counts = count_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9P64uoyX3KE"
      },
      "source": [
        "We will return to this later. First let us see what `CountVectorizer` produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1LDpV78X3KE"
      },
      "source": [
        "X_train_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-d5NODYX3KF"
      },
      "source": [
        "The vectorised form contains 11314 rows, which is the number of our documents, while the number of columns tells us something about the vocabulary size of the whole corpus. But what's a sparse matrix? Note that saving the complete, sparse document-vocabulary matrix would need to hold 1,472,030,598 values, most of which would be zero? Why? Instead, we only save 1,787,565 values in a compressed sparse row format. An example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIUos_j5X3KF"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "row = np.array([0, 0, 1, 2, 2, 2])\n",
        "col = np.array([0, 2, 2, 0, 1, 2])\n",
        "data = np.array([1, 2, 3, 4, 5, 6])\n",
        "mtx = sparse.csr_matrix((data, (row, col)), shape=(3, 3))\n",
        "mtx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMN2JG4AX3KF"
      },
      "source": [
        "m = mtx.todense()\n",
        "m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCUnpTCDX3KF"
      },
      "source": [
        "m[0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZpgzsfKX3KF"
      },
      "source": [
        "m[0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIBL28e0X3KG"
      },
      "source": [
        "How does indexing and printing of sparse matrices work?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ2BCNwrX3KG"
      },
      "source": [
        "print(mtx[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68k9dGPX3KG"
      },
      "source": [
        "Now let's apply our new knowledge to our word-document matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6i53jI-X3KG"
      },
      "source": [
        "X_train_counts.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap-exvLvX3KG"
      },
      "source": [
        "X_train_counts[0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn2DMP8HX3KH"
      },
      "source": [
        "We can see which positions of the document vector are occupied. A `1` means the word occurs once in the document, while any other number gives the exact count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVaZB53rX3KH"
      },
      "source": [
        "print(X_train_counts[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA0pHM24X3KH"
      },
      "source": [
        "The number of words in a document is also trivial to get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFXjDffNX3KH"
      },
      "source": [
        "X_train_counts[0,:].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBm5u0qCX3KH"
      },
      "source": [
        "In a similar fashion, we can count how many times a certain word occurs in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnhzqdTLX3KH"
      },
      "source": [
        "X_train_counts[:,0].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKwmL2JlX3KH"
      },
      "source": [
        "We can also learn more about the vocabulary, e.g., how many times a word occurs in the corpus. First, we need to find the index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XD2f5zPX3KI"
      },
      "source": [
        "count_vect.vocabulary_.get('sin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odmWGpRHX3KI"
      },
      "source": [
        "Now we have the index, we can count how many times the word \"sin\" occurs in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSdMHbO-X3KI"
      },
      "source": [
        "sin_index = count_vect.vocabulary_.get('sin')\n",
        "X_train_counts[:,sin_index].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlSBsNpoX3KI"
      },
      "source": [
        "So far, so good. `CountVectorizer` lets you also define if you want to count bigrams, or other n-grams. Moreover, you can not only count words, but als characters. We suggest you try these out for yourself. In the following, we will continue with unigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BuVj8CmX3KI"
      },
      "source": [
        "Since we have numbers now instead of strings, we could start training models now. However, raw counts will not be very informative, since we also have to take the length of a dodument into account. Dividing each row by the total number of words will give us the term frequency for each document. That will be much better! Now we still might have higher values for words which occur often in many documents. typically, these words are less informative, so we need to downscale those weights. This will modify or counts so that we are left with what is called the \"term frequency-inverse document frequency\" measure, or tf-idf. The tf-idf measure is given by\n",
        "\\begin{equation}\n",
        "f_{t,d}\\cdot log \\frac{N}{n_t}\n",
        "\\end{equation}\n",
        "In sklearn, there is the `TfidfTransformer` which does exactly that for us :-)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJXMA0q_X3KI"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_tranformer = TfidfTransformer(smooth_idf=True).fit(X_train_counts)\n",
        "X_train_tfidf = tfidf_tranformer.transform(X_train_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Y76iwGX3KI"
      },
      "source": [
        "X_train_tfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJqpPKhyX3KJ"
      },
      "source": [
        "X_train_tfidf[0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6UByJxUX3KJ"
      },
      "source": [
        "print(X_train_tfidf[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JRtlsIlX3KJ"
      },
      "source": [
        "Again we apply the transformation to the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGEiD2tWX3KJ"
      },
      "source": [
        "X_test_tfidf = tfidf_tranformer.transform(X_test_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyveNTbUX3KJ"
      },
      "source": [
        "This should suffice as features to train a classifer (for the moment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ6F8ZM1X3KJ"
      },
      "source": [
        "### Vectorise labels\n",
        "Next, we deal with the labels. Every document has exactly one label attached. We have 20 labels in total. This means we can basically assign a number to each label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhdcRdMOX3KJ"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yedDDaqeX3KK"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7QJT1iMX3KK"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0G3-3BOX3KK"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7QgLsXNX3KK"
      },
      "source": [
        "label_encoder.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOpydQgDX3KK"
      },
      "source": [
        "## Finally, let's train models\n",
        "Now it's time to train models. Let's stick to the Multinomial Naive Bayes classifier for the moment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5E49BmvX3KL"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY8jOFswX3KL"
      },
      "source": [
        "Let's see how well we do on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qSKvif7X3KL"
      },
      "source": [
        "nb_clf.predict(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veDcE5vrX3KL"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSrwY3o8X3KL"
      },
      "source": [
        "Computing the accuracy is simple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCBBI7ACX3KM"
      },
      "source": [
        "correct = 0\n",
        "\n",
        "for index, prediction in enumerate(nb_clf.predict(X_test_tfidf)):\n",
        "    if prediction == y_test[index]:\n",
        "        correct +=1\n",
        "\n",
        "print('Accuracy: ', correct/y_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwKSxJ4xX3KM"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(nb_clf.predict(X_test_tfidf), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcTQCrM_X3KM"
      },
      "source": [
        "Almost 80 percent, that is not too bad. What about a Support Vector Classifier?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI33TO6QX3KM"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJEbYM4ZX3KM"
      },
      "source": [
        "correct = 0\n",
        "\n",
        "for index, prediction in enumerate(svc.predict(X_test_tfidf)):\n",
        "    if prediction == y_test[index]:\n",
        "        correct +=1\n",
        "\n",
        "print('Accuracy: ', correct/y_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxGpuK1pX3KM"
      },
      "source": [
        "An increase of 8%, that's good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qckHMB-X3KM"
      },
      "source": [
        "However, in order to determine the performance of our models we need cross validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIVCpA-rX3KN"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(nb_clf, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADvb5_dwX3KN"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgM4c9W-X3KN"
      },
      "source": [
        "scores = cross_val_score(svc, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WanFN3RLX3KN"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1_rQidWX3KN"
      },
      "source": [
        "We can also calculate precision, recall, and f1 relatively easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H04oOtkMX3KN"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(max_iter=50)\n",
        "sgd_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_train_predictions = cross_val_predict(sgd_clf, X_train_tfidf, y_train, cv=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUrvEIDKX3KN"
      },
      "source": [
        "print(precision_score(y_train, y_train_predictions, average='micro'))\n",
        "print(recall_score(y_train, y_train_predictions, average='micro'))\n",
        "print(f1_score(y_train, y_train_predictions, average='micro'))\n",
        "conf_mx = confusion_matrix(y_train, y_train_predictions)\n",
        "conf_mx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzndJUMpX3KO"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCMeQ-TZX3KO"
      },
      "source": [
        "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
        "norm_conf_mx = conf_mx / row_sums\n",
        "\n",
        "np.fill_diagonal(norm_conf_mx, 0)\n",
        "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fOZGvBUoX3KO"
      },
      "source": [
        "label_encoder.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br-EQ2AoX3KO"
      },
      "source": [
        "## Shortcuts in sklearn - pipelines\n",
        "Sklearn allows us to build convenient `Pipelines`, which facilitate the management of our data and the training of our models enourmously. Consider for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbTKdcoeX3KO"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('nb_clf', MultinomialNB())\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx1_FDKcX3KO"
      },
      "source": [
        "We could even replace the two first lines of the pipeline by using `TfidfVectorizer`, which first fits and transforms the input the same way as the `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCysE7tLX3KO"
      },
      "source": [
        "text_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmX9mh6eX3KP"
      },
      "source": [
        "scores = cross_val_score(text_clf, X_train, y_train, scoring='accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBKGQzJLX3KP"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWKm1CyaX3KP"
      },
      "source": [
        "## Model selection - find your best model\n",
        "For every model you would like to train, there is a plethora of parameters you could set. How to find the best model? Again, sklearn has a solution: `GridSearchCV`. With grid search cross validation, you can set your hyperparameter space and train different models with all the parameter combinations. Keep in mind that depending on how many folds you train, the whole training procedure takes significantly longer. But let's set up grid search cross validation. We set up a new pipeline for a SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuyNDDbjX3KP"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "text_svc = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('svc', LinearSVC())\n",
        "])\n",
        "\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "             'svc__loss': ['hinge', 'squared_hinge'],\n",
        "             'svc__multi_class': ['ovr', 'crammer_singer']}\n",
        "\n",
        "gs_svc = GridSearchCV(text_svc, param_grid, cv=5, n_jobs=4, verbose=1)\n",
        "gs_svc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd3PtMjYX3KP"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "text_svc = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('svc', LinearSVC())\n",
        "])\n",
        "\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "             'svc__loss': ['hinge', 'squared_hinge'],\n",
        "             'svc__multi_class': ['ovr', 'crammer_singer']}\n",
        "\n",
        "gs_svc = GridSearchCV(text_svc, param_grid, cv=10, n_jobs=3, verbose=1, return_train_score=True)\n",
        "gs_svc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyKZRLITX3KP"
      },
      "source": [
        "svc_df = pd.DataFrame.from_dict(gs_svc.cv_results_)\n",
        "svc_df.sort_values(by=[\"rank_test_score\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2OuEAVLX3KP"
      },
      "source": [
        "gs_svc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-HQRx4kX3KQ"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyPQWIOzX3KQ"
      },
      "source": [
        "best_model = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('svc', LinearSVC(loss='hinge', multi_class='crammer_singer'))\n",
        "])\n",
        "\n",
        "best_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIrLAwyyX3KQ"
      },
      "source": [
        "best_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCF056HrX3KQ"
      },
      "source": [
        "correct = 0\n",
        "\n",
        "for index, prediction in enumerate(best_model.predict(X_test)):\n",
        "    if prediction == y_test[index]:\n",
        "        correct +=1\n",
        "\n",
        "print('Accuracy: ', correct/y_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Modern Solutions Sneak Peek - Transformer\n",
        "\n",
        "Let's look at another task, paraphrase detection. Do two sentences have the same meaning?"
      ],
      "metadata": {
        "id": "JBLEzh-MCVfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwhWBd2aEqeF",
        "outputId": "32b3ee12-ed1a-4151-93cd-1bdf2f771f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# 1. Load Pre-trained Model and Tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"AMHR/adversarial-paraphrasing-detector\"  # Replace with the model you want to use\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 2. Prepare the Dataset\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "eval_dataset = dataset[\"validation\"]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# 3. Evaluate the Model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, example in enumerate(eval_dataset):\n",
        "        if i > 5:\n",
        "           break\n",
        "        # Tokenize the inputs and get the model's predictions\n",
        "        inputs = tokenizer(example['sentence1'], example['sentence2'], return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "        # Move input tensors to the same device as the model\n",
        "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        # Print first 5 example pairs along with predictions and ground truth labels\n",
        "        if i < 5:\n",
        "            print(f\"Example {i+1}\")\n",
        "            print(f\"Sentence 1: {example['sentence1']}\")\n",
        "            print(f\"Sentence 2: {example['sentence2']}\")\n",
        "            print(f\"Prediction: {'Paraphrase' if predictions == 1 else 'Not a Paraphrase'}\")\n",
        "            print(f\"Ground Truth: {'Paraphrase' if example['label'] == 1 else 'Not a Paraphrase'}\")\n",
        "            print(\"=\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGNDxHAlEHY0",
        "outputId": "f44fc39a-5549-4a59-fb7f-3f75e7e0b971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1\n",
            "Sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\n",
            "Sentence 2: \" The foodservice pie business does not fit our long-term growth strategy .\n",
            "Prediction: Paraphrase\n",
            "Ground Truth: Paraphrase\n",
            "==================================================\n",
            "Example 2\n",
            "Sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war .\n",
            "Sentence 2: His wife said he was \" 100 percent behind George Bush \" and looked forward to using his years of training in the war .\n",
            "Prediction: Not a Paraphrase\n",
            "Ground Truth: Not a Paraphrase\n",
            "==================================================\n",
            "Example 3\n",
            "Sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat .\n",
            "Sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .\n",
            "Prediction: Not a Paraphrase\n",
            "Ground Truth: Not a Paraphrase\n",
            "==================================================\n",
            "Example 4\n",
            "Sentence 1: The AFL-CIO is waiting until October to decide if it will endorse a candidate .\n",
            "Sentence 2: The AFL-CIO announced Wednesday that it will decide in October whether to endorse a candidate before the primaries .\n",
            "Prediction: Not a Paraphrase\n",
            "Ground Truth: Paraphrase\n",
            "==================================================\n",
            "Example 5\n",
            "Sentence 1: No dates have been set for the civil or the criminal trial .\n",
            "Sentence 2: No dates have been set for the criminal or civil cases , but Shanley has pleaded not guilty .\n",
            "Prediction: Not a Paraphrase\n",
            "Ground Truth: Not a Paraphrase\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_paraphrase(sentence1, sentence2, model, tokenizer, device):\n",
        "    # Prepare the sentences for the model\n",
        "    inputs = tokenizer(sentence1, sentence2, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # Move the input tensors to the device the model is on\n",
        "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get model's prediction\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    return \"Paraphrase\" if prediction == 1 else \"Not a Paraphrase\""
      ],
      "metadata": {
        "id": "Mh0ryN2WG-dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Sentences testing\n",
        "sentence1 = \"This tutorial rocks.\"\n",
        "sentence2 = \"I want to throw rocks at this Tutor.\"\n",
        "\n",
        "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
        "print(f\"Sentence 1: {sentence1}\")\n",
        "print(f\"Sentence 2: {sentence2}\")\n",
        "print(f\"Prediction: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpuHVRgOGvxL",
        "outputId": "b07a703c-9f19-403c-8404-55035cabe948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: This tutorial rocks.\n",
            "Sentence 2: I want to throw rocks at this Tutor.\n",
            "Prediction: Not a Paraphrase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Sentences Testing\n",
        "sentence1 = \"I am so tired but I want to stay in this tutorial.\"\n",
        "sentence2 = \"I am exhausted and forced to be here.\"\n",
        "\n",
        "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
        "print(f\"Sentence 1: {sentence1}\")\n",
        "print(f\"Sentence 2: {sentence2}\")\n",
        "print(f\"Prediction: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RibEJxxdICU7",
        "outputId": "62756b05-e77a-4220-c7d4-899f5d6605c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: I am so tired but I want to stay in this tutorial.\n",
            "Sentence 2: I am exhausted and forced to be here.\n",
            "Prediction: Not a Paraphrase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Sentences Testing\n",
        "sentence1 = \"This field of research is pretty cool.\"\n",
        "sentence2 = \"I find this line of research very cool.\"\n",
        "\n",
        "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
        "print(f\"Sentence 1: {sentence1}\")\n",
        "print(f\"Sentence 2: {sentence2}\")\n",
        "print(f\"Prediction: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6HGgabNIcsy",
        "outputId": "4f4310d1-00b5-4948-e0c1-4716c44cf156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: This field of research is pretty cool.\n",
            "Sentence 2: I find this line of research very cool.\n",
            "Prediction: Paraphrase\n"
          ]
        }
      ]
    }
  ]
}