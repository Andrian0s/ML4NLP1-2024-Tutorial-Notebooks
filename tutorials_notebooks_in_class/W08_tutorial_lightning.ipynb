{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2023-Tutorial-Notebooks/blob/main/tutorial_notebooks/08_tutorial_lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyrhwMu0BoJZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.utils.data as data\n",
        "# import torch.utils.data as data, torchvision as tv\n",
        "import lightning as L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNNEe9D1CvNW"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank('en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPbA2ZrREFNC",
        "outputId": "2360709d-ea0f-480a-9408-9c3e9d233b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 7 GPU(s) available.\n",
            "Device name: NVIDIA GeForce GTX TITAN X\n"
          ]
        }
      ],
      "source": [
        "# use the GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZV7yzsYDk5D"
      },
      "outputs": [],
      "source": [
        "# import the dataset (txt file) line by line\n",
        "def load_text(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        texts = []\n",
        "        for line in f:\n",
        "            texts.append(line.decode(errors='ignore').lower().strip())\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxRgXT3vD2ix"
      },
      "outputs": [],
      "source": [
        "neg_text = load_text(\"movie_review_data/data/rt-polarity.neg\")\n",
        "pos_text = load_text(\"movie_review_data/data/rt-polarity.pos\")\n",
        "# concat negative and positive texts\n",
        "texts = neg_text + pos_text\n",
        "# we know the order in texts variable, so we can label it accordingly\n",
        "labels = np.array([0]*len(neg_text) + [1]*len(pos_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqP3wWAbffH_"
      },
      "outputs": [],
      "source": [
        "def tokenize(texts):\n",
        "  \"\"\"\n",
        "  Assign unique id to each token\n",
        "  \"\"\"\n",
        "  max_len = 0\n",
        "  tokenized_texts = []\n",
        "  word2idx = {}\n",
        "\n",
        "  # Add <pad> and <unk> tokens to the vocabulary\n",
        "  word2idx['<pad>'] = 0\n",
        "  word2idx['<unk>'] = 1\n",
        "\n",
        "  # Building our vocab from the corpus starting from index 2\n",
        "  idx = 2\n",
        "  for sent in texts:\n",
        "    tokenized_sent = nlp(sent)\n",
        "    # Add `tokenized_sent` to `tokenized_texts`\n",
        "    tokenized_texts.append(tokenized_sent)\n",
        "    # Add new token to `word2idx`\n",
        "    for token in tokenized_sent:\n",
        "      # string any token objects are different things, be careful.\n",
        "      if token.text not in word2idx:\n",
        "        word2idx[token.text] = idx\n",
        "        idx += 1\n",
        "\n",
        "        # Update `max_len`\n",
        "    max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "  return tokenized_texts, word2idx, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N0qAlGJicxG"
      },
      "outputs": [],
      "source": [
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "        # Pad sentences to max_len\n",
        "        tokenized_padded_sent = list(tokenized_sent) + ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        # Encode tokens to input_ids\n",
        "        input_id = [word2idx.get(str(token)) for token in tokenized_padded_sent]\n",
        "        input_ids.append(input_id)\n",
        "\n",
        "    return np.array(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0GvkEDscK41"
      },
      "outputs": [],
      "source": [
        "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqwxTRS-rzEg"
      },
      "outputs": [],
      "source": [
        "# Convert data type to torch.Tensor\n",
        "train_inputs = torch.from_numpy(input_ids)\n",
        "labels = torch.from_numpy(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQjwRt6FEFNG"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "# Create DataLoader for training data\n",
        "all_data = TensorDataset(train_inputs, labels)\n",
        "dataset = TensorDataset(train_inputs, labels)\n",
        "total_len = len(dataset)\n",
        "train_size = int(0.7 * total_len)\n",
        "val_size = int(0.2 * total_len)\n",
        "test_size = total_len - train_size - val_size\n",
        "\n",
        "train_data, val_data, test_data = data.random_split(dataset, [train_size, val_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJZwf9_hEFNH"
      },
      "outputs": [],
      "source": [
        "# Specify batch_size\n",
        "batch_size = 8\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data)\n",
        "test_dataloader = DataLoader(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYVSgmILEFNH"
      },
      "outputs": [],
      "source": [
        "train_classes = [label for _, label in train_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIzU9ASmO7Of"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size=len(word2idx),\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[3, 4, 5],\n",
        "                 num_filters=[100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "        The constructor for CNN class.\n",
        "        Args:\n",
        "            vocab_size (int): Need to be specified when pretrained word\n",
        "                embeddings are not used.\n",
        "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
        "            num_filters (List[int]): List of number of filters, has the same\n",
        "                length as `filter_sizes`. Default: [100, 100, 100]\n",
        "            n_classes (int): Number of classes. Default: 2\n",
        "            dropout (float): Dropout rate. Default: 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx=0,\n",
        "                                          max_norm=5.0)\n",
        "        # Conv Network\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"Perform a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
        "                (batch_size, max_sent_length)\n",
        "\n",
        "        Returns:\n",
        "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
        "                n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings from `input_ids`. Output shape: (batch_size, max_len, embed_dim)\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        # Output shape: (b, embed_dim, max_len)\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "\n",
        "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list]\n",
        "\n",
        "        # Concatenate x_pool_list to feed the fully connected layer.\n",
        "        # Output shape: (b, sum(num_filters))\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
        "                         dim=1)\n",
        "\n",
        "        # Compute logits. Output shape: (b, n_classes)\n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_naLbLXEFNH"
      },
      "outputs": [],
      "source": [
        "conf = {\n",
        "    'vocab_size': len(word2idx),\n",
        "    'embed_dim': 300,\n",
        "    'filter_sizes': [3, 4, 5],\n",
        "    'num_filters': [200, 200, 200],\n",
        "    'num_classes': 2,\n",
        "    'dropout': 0.5\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzvvWB_kEFNI"
      },
      "outputs": [],
      "source": [
        "import torch.optim\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "class CNNLit(L.LightningModule):\n",
        "    def __init__(self, conf, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters(conf)\n",
        "        self.model = CNN(\n",
        "            vocab_size=self.hparams['vocab_size'],\n",
        "            embed_dim=self.hparams['embed_dim'],\n",
        "            filter_sizes=self.hparams['filter_sizes'],\n",
        "            num_filters=self.hparams['num_filters'],\n",
        "            num_classes=self.hparams['num_classes'],\n",
        "            dropout=self.hparams['dropout']\n",
        "        )\n",
        "        # Create model\n",
        "        # self.model = create_model(model_name, model_hparams)\n",
        "        \"\"\"\n",
        "        self.model = CNN(embed_dim=300,\n",
        "            filter_sizes=[3, 4, 5],\n",
        "            num_filters=[100, 100, 100],\n",
        "            num_classes=2,\n",
        "            dropout=0.5)\n",
        "        \"\"\"\n",
        "        # Create loss module\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # in lightning, forward defines the prediction/inference actions\n",
        "        output = self.encoder(x)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # \"batch\" is the output of the training data loader.\n",
        "        input_ids, labels = batch\n",
        "        preds = self.model(input_ids)\n",
        "        loss = self.loss_module(preds, labels)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss  # Return tensor to call \".backward\" on\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids, labels = batch\n",
        "        preds = self.model(input_ids)\n",
        "        loss = self.loss_module(preds, labels)\n",
        "        # flat_preds = preds.clone().detach().argmax(dim=-1)\n",
        "        flat_preds = torch.argmax(preds, axis=1).flatten().cpu()\n",
        "        accuracy = Accuracy(task='binary')\n",
        "        acc = accuracy(flat_preds, labels.cpu())\n",
        "        self.log('val_acc', acc)\n",
        "        self.log(\"val_loss\", loss)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids, labels = batch\n",
        "        preds = self.model(input_ids)\n",
        "        flat_preds = np.argmax(preds.cpu(), axis=1).flatten()\n",
        "        accuracy = Accuracy(task='binary')\n",
        "        acc = accuracy(flat_preds, labels.cpu())\n",
        "        self.log(\"test_acc\", acc)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASIIrImWVhEC",
        "outputId": "ebeab359-417e-456b-e7d5-0d1a7a6e959a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ]
        }
      ],
      "source": [
        "L.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:3\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwo_zrudEFNI",
        "outputId": "a46a7f0f-048c-4099-e9f6-e4b57e02adf4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "cnn = CNNLit(conf)\n",
        "trainer = L.Trainer(\n",
        "        # We run on a single GPU (if possible)\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        # How many epochs to train for if no patience is set\n",
        "        max_epochs=10,\n",
        "        callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")],\n",
        "        auto_lr_find=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "b459410d055a42ddb195f61a61629765",
            "618322f744d04eba982e25ef02b9800d",
            "9467ef49137d4b12bb898729288efcac",
            "493c197511984617817fd65343673ae1",
            "389f8ee23731473b87f8b2d48a419490",
            "7ded436e2fe445b89179405f9936eed1"
          ]
        },
        "id": "1asuMytjXK8c",
        "outputId": "4d0bc002-0f6f-42c5-9b1a-26f34352a228"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
            "\n",
            "  | Name        | Type             | Params\n",
            "-------------------------------------------------\n",
            "0 | model       | CNN              | 6.3 M \n",
            "1 | loss_module | CrossEntropyLoss | 0     \n",
            "-------------------------------------------------\n",
            "6.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "6.3 M     Total params\n",
            "25.097    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b459410d055a42ddb195f61a61629765",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/haller/anaconda3/envs/measure-uid/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  category=PossibleUserWarning,\n",
            "/home/user/haller/anaconda3/envs/measure-uid/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  category=PossibleUserWarning,\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "618322f744d04eba982e25ef02b9800d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9467ef49137d4b12bb898729288efcac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "493c197511984617817fd65343673ae1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "389f8ee23731473b87f8b2d48a419490",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ded436e2fe445b89179405f9936eed1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.fit(cnn, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCoSaqd3EFNJ",
        "outputId": "cde8481c-ea31-4d09-e791-60dffd75d5cf",
        "colab": {
          "referenced_widgets": [
            "a883ed13e5424547a69617f41f554e52",
            "5be702f6cd06440daa630227fe2c531f"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/haller/anaconda3/envs/measure-uid/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  category=PossibleUserWarning,\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a883ed13e5424547a69617f41f554e52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5be702f6cd06440daa630227fe2c531f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "    val_result = trainer.test(cnn, dataloaders=val_dataloader, verbose=False)\n",
        "    test_result = trainer.test(cnn, dataloaders=test_dataloader, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-VyAT0HEFNJ",
        "outputId": "9fa9a4d9-0c0b-4f6e-dce0-23c081aa8989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'test': 0.7375820279121399, 'val': 0.73968106508255}\n"
          ]
        }
      ],
      "source": [
        "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
        "    print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}