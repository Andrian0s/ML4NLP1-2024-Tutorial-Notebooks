{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2023-Tutorial-Notebooks/blob/main/tutorial_notebooks/02_intro_to_sklearn_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gzHo80DX3J0"
      },
      "source": [
        "# Introduction to scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVB8QC6bX3J5"
      },
      "source": [
        "We would like to introduce you to scikit-learn with the help of an instructional example about text classification. We will cover the most basic principles and ideas about scikit-learn in this notebook. This tutorial is inspired by the sklearn tutorial on http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html, but contains a few more explanations and is suited to introduce scikit-learn in class.\n",
        "\n",
        "$Author$: Phillip Str√∂bel\n",
        "\n",
        "With adjustments from: Janis Goldzycher, Andrianos Michail\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuLQDR5MX3J6"
      },
      "source": [
        "## Data\n",
        "\n",
        "Get the data from http://qwone.com/~jason/20Newsgroups/. We will work with the 20news-bydate.tar.gz data set. Unzip it to a suitable destination. Here, all the data lies in the data folder. To our convenience, it has already been split into a training and a test set, so we don't have to care about this. What we need to do though is get the data and put it into a dataframe (you could also only work with dictionaries or other data containers). We do this for both the training and the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuV1TvUNX3J7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_df(path_to_data, random_state=42):\n",
        "    \"\"\"\n",
        "    Takes the path of a folder containing all the subfolders (which contain the actual documents).\n",
        "    Builds a pandas datafram with document ids, the text and the label.\n",
        "    :param path_to_data: path to top folder as a string\n",
        "    :param random_state: integer, seed for shuffling\n",
        "    :return: pandas dataframe with all th\n",
        "    \"\"\"\n",
        "    doc_list = list()  # doc_list now: [[doc<str>, label<str>], ...]\n",
        "\n",
        "    for category in os.listdir(path_to_data):\n",
        "        for document in os.listdir(os.path.join(path_to_data, category)):\n",
        "            doc = open(os.path.join(path_to_data, category, document), 'r', encoding='latin-1').read().replace('\\n', ' ')\n",
        "            doc_list.append([doc, category])\n",
        "\n",
        "    df = pd.DataFrame(doc_list, columns=['text', 'label'])\n",
        "\n",
        "    return df.sample(frac=1, random_state=random_state) # return and shuffle dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhNP5KxyX3J8"
      },
      "outputs": [],
      "source": [
        "train = create_df('data/20news-bydate/20news-bydate-train')\n",
        "test = create_df('data/20news-bydate/20news-bydate-test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuvWeXL2X3J8"
      },
      "source": [
        "Several ways to inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSiZBH2nX3J9",
        "outputId": "e4fe6645-3622-4bdf-ba4a-1ff8d60e7bab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training size:  (11314, 2)\n",
            "test size:  (7532, 2)\n"
          ]
        }
      ],
      "source": [
        "print('training size: ', train.shape)\n",
        "print('test size: ', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1hM5sIuX3J-",
        "outputId": "3a090399-6064-4240-8f70-a66e5727627a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7492</th>\n",
              "      <td>From: prb@access.digex.com (Pat) Subject: Re: ...</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3546</th>\n",
              "      <td>From: jcj@tellabs.com (jcj) Subject: Re: Losin...</td>\n",
              "      <td>soc.religion.christian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5582</th>\n",
              "      <td>From: hacker@cco.caltech.edu (Jonathan Bruce H...</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4793</th>\n",
              "      <td>From: harmons@.WV.TEK.COM (Harmon Sommer) Subj...</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3813</th>\n",
              "      <td>From: dppeak@matt.ksu.ksu.edu (David Paul Peak...</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  \\\n",
              "7492  From: prb@access.digex.com (Pat) Subject: Re: ...   \n",
              "3546  From: jcj@tellabs.com (jcj) Subject: Re: Losin...   \n",
              "5582  From: hacker@cco.caltech.edu (Jonathan Bruce H...   \n",
              "4793  From: harmons@.WV.TEK.COM (Harmon Sommer) Subj...   \n",
              "3813  From: dppeak@matt.ksu.ksu.edu (David Paul Peak...   \n",
              "\n",
              "                       label  \n",
              "7492               sci.space  \n",
              "3546  soc.religion.christian  \n",
              "5582               rec.autos  \n",
              "4793         rec.motorcycles  \n",
              "3813   comp.sys.mac.hardware  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63bHHHJEX3J_",
        "outputId": "80e73c38-b6af-4a33-ed3e-9a3602853233",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 11314 entries, 7492 to 7270\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    11314 non-null  object\n",
            " 1   label   11314 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 265.2+ KB\n"
          ]
        }
      ],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfXDPPx2X3KA",
        "outputId": "66bb7cdc-6c58-44c6-9df8-7cf518a661cb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11314</td>\n",
              "      <td>11314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>11314</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>From: gt6511a@prism.gatech.EDU (COCHRANE,JAMES...</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text             label\n",
              "count                                               11314             11314\n",
              "unique                                              11314                20\n",
              "top     From: gt6511a@prism.gatech.EDU (COCHRANE,JAMES...  rec.sport.hockey\n",
              "freq                                                    1               600"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXZ8GGUIX3KA",
        "outputId": "e0a85072-125c-4b17-bccc-61c93bea3d8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "alt.atheism                 480\n",
              "comp.graphics               584\n",
              "comp.os.ms-windows.misc     591\n",
              "comp.sys.ibm.pc.hardware    590\n",
              "comp.sys.mac.hardware       578\n",
              "comp.windows.x              593\n",
              "misc.forsale                585\n",
              "rec.autos                   594\n",
              "rec.motorcycles             598\n",
              "rec.sport.baseball          597\n",
              "rec.sport.hockey            600\n",
              "sci.crypt                   595\n",
              "sci.electronics             591\n",
              "sci.med                     594\n",
              "sci.space                   593\n",
              "soc.religion.christian      599\n",
              "talk.politics.guns          546\n",
              "talk.politics.mideast       564\n",
              "talk.politics.misc          465\n",
              "talk.religion.misc          377\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.groupby('label').size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIz2S4PX3KA"
      },
      "source": [
        "As usual, we split the labels from the training and the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r6WywPEX3KB"
      },
      "outputs": [],
      "source": [
        "X_train = train.text\n",
        "y_train = train.label\n",
        "X_test = test.text\n",
        "y_test = test.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCaHvzBVX3KB",
        "outputId": "73199120-60b3-46d6-e9e8-924de4593f71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmuRdUSSX3KC"
      },
      "source": [
        "Series is just a \"One-dimensional ndarray with axis labels\". Let's see if we got this right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3PoWkuRX3KC",
        "outputId": "8f5faec4-8ae5-4195-caa9-cc320f0e7962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape:  (11314,)\n",
            "Training labels shape:  (11314,)\n",
            "Test set shape:  (7532,)\n",
            "Test labels shape:  (7532,)\n"
          ]
        }
      ],
      "source": [
        "print('Training set shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test set shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJvHvu5pX3KD",
        "outputId": "9f6a125b-7d94-45e0-c823-5b82c941ec24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7492    From: prb@access.digex.com (Pat) Subject: Re: ...\n",
              "3546    From: jcj@tellabs.com (jcj) Subject: Re: Losin...\n",
              "5582    From: hacker@cco.caltech.edu (Jonathan Bruce H...\n",
              "4793    From: harmons@.WV.TEK.COM (Harmon Sommer) Subj...\n",
              "3813    From: dppeak@matt.ksu.ksu.edu (David Paul Peak...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyUIWl1BX3KD"
      },
      "source": [
        "## Preprocessing\n",
        "So far, so good! But we know that machine learning algorithms cannot work with text data directly. So we need to vectorise the data somehow. also, we might do some preprocessing. Let's see how we can tackle these problems.\n",
        "### Vectorise the data\n",
        "Luckily, sklearn offers some nice classes which help us. We should tokenise the data and then vectorise it. Conveniently, sklearns `CountVectoriser()` does exactly that. Let's see how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_T0KafxX3KD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)  # num_docs x num_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jINuDaIX3KD"
      },
      "source": [
        "Basically, the three central methods in sklearn are `transform`, `fit`, `fit_transform`, and `predict`. We will see how each of these work and when to use them. We have alredy made use of `fit_transform`. Instead of using this method, we could have called the method `fit` on the training set first and the use `transform` to vectorise the data (to 'transform' it). With the fitted `CountVectorizer` we can now transform other data, like for example the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Ma6B6DX3KE"
      },
      "outputs": [],
      "source": [
        "X_test_counts = count_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9P64uoyX3KE"
      },
      "source": [
        "We will return to this later. First let us see what `CountVectorizer` produces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1LDpV78X3KE",
        "outputId": "f74dae93-35ea-4f8b-b56e-3819d4f39c5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1787565 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-d5NODYX3KF"
      },
      "source": [
        "The vectorised form contains 11314 rows, which is the number of our documents, while the number of columns tells us something about the vocabulary size of the whole corpus. But what's a sparse matrix? Note that saving the complete, sparse document-vocabulary matrix would need to hold 1,472,030,598 values, most of which would be zero? Why? Instead, we only save 1,787,565 values in a compressed sparse row format. An example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIUos_j5X3KF",
        "outputId": "f56ea9cb-0435-4c9f-ea32-0f7233679398"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<3x3 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 6 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "row = np.array([0, 0, 1, 2, 2, 2])\n",
        "col = np.array([0, 2, 2, 0, 1, 2])\n",
        "data = np.array([1, 2, 3, 4, 5, 6])\n",
        "mtx = sparse.csr_matrix((data, (row, col)), shape=(3, 3))\n",
        "mtx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMN2JG4AX3KF",
        "outputId": "f222d09c-bdcd-4f4e-a9fa-599dcb043d3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[1, 0, 2],\n",
              "        [0, 0, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m = mtx.todense()\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCUnpTCDX3KF",
        "outputId": "749ec602-a5c6-44ee-e6fc-fc235ac50e28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m[0,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZpgzsfKX3KF",
        "outputId": "316c84bf-23ca-4b24-af21-12dfdd89e79e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[1, 0, 2]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m[0,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIBL28e0X3KG"
      },
      "source": [
        "How does indexing and printing of sparse matrices work?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ2BCNwrX3KG",
        "outputId": "883fe8d6-8e4a-4b06-d8b7-0740ef8f774d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 0)\t1\n",
            "  (2, 0)\t4\n"
          ]
        }
      ],
      "source": [
        "print(mtx[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68k9dGPX3KG"
      },
      "source": [
        "Now let's apply our new knowledge to our word-document matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6i53jI-X3KG",
        "outputId": "f108b47e-a84d-43d2-d1bf-f17bb2499fd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11314, 130107)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_counts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap-exvLvX3KG",
        "outputId": "ea56dc71-1ff4-4c7b-acd9-aa7b4a677fc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 60 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_counts[0,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn2DMP8HX3KH"
      },
      "source": [
        "We can see which positions of the document vector are occupied. A `1` means the word occurs once in the document, while any other number gives the exact count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVaZB53rX3KH",
        "outputId": "60396dfc-423a-405e-f4db-12211187cec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 56979)\t1\n",
            "  (0, 95455)\t1\n",
            "  (0, 25605)\t3\n",
            "  (0, 47246)\t2\n",
            "  (0, 41105)\t1\n",
            "  (0, 92305)\t2\n",
            "  (0, 111322)\t1\n",
            "  (0, 99721)\t1\n",
            "  (0, 107159)\t1\n",
            "  (0, 91242)\t1\n",
            "  (0, 123292)\t1\n",
            "  (0, 35456)\t1\n",
            "  (0, 29987)\t1\n",
            "  (0, 90379)\t1\n",
            "  (0, 53521)\t1\n",
            "  (0, 89917)\t1\n",
            "  (0, 41316)\t1\n",
            "  (0, 119714)\t1\n",
            "  (0, 76032)\t1\n",
            "  (0, 2927)\t1\n",
            "  (0, 87620)\t1\n",
            "  (0, 95162)\t1\n",
            "  (0, 64095)\t1\n",
            "  (0, 86694)\t1\n",
            "  (0, 114808)\t1\n",
            "  :\t:\n",
            "  (0, 28601)\t1\n",
            "  (0, 59779)\t1\n",
            "  (0, 107705)\t1\n",
            "  (0, 27001)\t1\n",
            "  (0, 65798)\t2\n",
            "  (0, 68766)\t2\n",
            "  (0, 42876)\t1\n",
            "  (0, 113279)\t2\n",
            "  (0, 90252)\t1\n",
            "  (0, 18888)\t1\n",
            "  (0, 114520)\t1\n",
            "  (0, 28012)\t1\n",
            "  (0, 90282)\t1\n",
            "  (0, 128402)\t1\n",
            "  (0, 71079)\t1\n",
            "  (0, 86493)\t2\n",
            "  (0, 122107)\t1\n",
            "  (0, 113172)\t1\n",
            "  (0, 48546)\t1\n",
            "  (0, 73201)\t2\n",
            "  (0, 114646)\t1\n",
            "  (0, 66184)\t1\n",
            "  (0, 28615)\t1\n",
            "  (0, 56283)\t1\n",
            "  (0, 112031)\t1\n"
          ]
        }
      ],
      "source": [
        "print(X_train_counts[0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA0pHM24X3KH"
      },
      "source": [
        "The number of words in a document is also trivial to get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFXjDffNX3KH",
        "outputId": "d2f2d616-73d3-4a02-dc14-18854cc4fbcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_counts[0,:].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBm5u0qCX3KH"
      },
      "source": [
        "In a similar fashion, we can count how many times a certain word occurs in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnhzqdTLX3KH",
        "outputId": "d57eebd6-e09e-43ea-dbaf-3dfb24d4189a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1534"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_counts[:,0].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKwmL2JlX3KH"
      },
      "source": [
        "We can also learn more about the vocabulary, e.g., how many times a word occurs in the corpus. First, we need to find the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XD2f5zPX3KI",
        "outputId": "498030b1-ee8b-44bc-cb64-88fecce85147"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "107529"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vect.vocabulary_.get('sin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odmWGpRHX3KI"
      },
      "source": [
        "Now we have the index, we can count how many times the word \"sin\" occurs in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSdMHbO-X3KI",
        "outputId": "cdab85e0-38bd-478a-e96c-ff502cedf156"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "284"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sin_index = count_vect.vocabulary_.get('sin')\n",
        "X_train_counts[:,sin_index].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlSBsNpoX3KI"
      },
      "source": [
        "So far, so good. `CountVectorizer` lets you also define if you want to count bigrams, or other n-grams. Moreover, you can not only count words, but als characters. We suggest you try these out for yourself. In the following, we will continue with unigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BuVj8CmX3KI"
      },
      "source": [
        "Since we have numbers now instead of strings, we could start training models now. However, raw counts will not be very informative, since we also have to take the length of a dodument into account. Dividing each row by the total number of words will give us the term frequency for each document. That will be much better! Now we still might have higher values for words which occur often in many documents. typically, these words are less informative, so we need to downscale those weights. This will modify or counts so that we are left with what is called the \"term frequency-inverse document frequency\" measure, or tf-idf. The tf-idf measure is given by\n",
        "\\begin{equation}\n",
        "f_{t,d}\\cdot log \\frac{N}{n_t}\n",
        "\\end{equation}\n",
        "In sklearn, there is the `TfidfTransformer` which does exactly that for us :-)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJXMA0q_X3KI"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_tranformer = TfidfTransformer(smooth_idf=True).fit(X_train_counts)\n",
        "X_train_tfidf = tfidf_tranformer.transform(X_train_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7Y76iwGX3KI",
        "outputId": "536ee9fd-1305-4770-e52d-a2a6d639e6fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11314, 130107)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_tfidf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJqpPKhyX3KJ",
        "outputId": "e9afdec0-7107-4890-e8d0-cef96a45888d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 60 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_tfidf[0,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6UByJxUX3KJ",
        "outputId": "e9265411-968e-41bc-9e86-ac6f428494ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 128402)\t0.040784535077023294\n",
            "  (0, 123292)\t0.050047475590731784\n",
            "  (0, 122107)\t0.23608192298840142\n",
            "  (0, 119714)\t0.07875000724650627\n",
            "  (0, 118561)\t0.08954168153621775\n",
            "  (0, 114808)\t0.09573115745960263\n",
            "  (0, 114646)\t0.050777861981411015\n",
            "  (0, 114520)\t0.06830757439166886\n",
            "  (0, 114455)\t0.02820701338545796\n",
            "  (0, 114440)\t0.03549606475525584\n",
            "  (0, 113279)\t0.16387685991792555\n",
            "  (0, 113172)\t0.22131647570679347\n",
            "  (0, 112031)\t0.08351647703105329\n",
            "  (0, 111322)\t0.02638497269006967\n",
            "  (0, 111152)\t0.19361694762246492\n",
            "  (0, 107705)\t0.11802257739846389\n",
            "  (0, 107159)\t0.15195563761417366\n",
            "  (0, 99721)\t0.036210478864209125\n",
            "  (0, 97123)\t0.09222812737895975\n",
            "  (0, 95455)\t0.15737149958892138\n",
            "  (0, 95162)\t0.04747820199185726\n",
            "  (0, 92305)\t0.2695561571516042\n",
            "  (0, 91242)\t0.18880639832564547\n",
            "  (0, 90379)\t0.0274484609010517\n",
            "  (0, 90282)\t0.1911021911312422\n",
            "  :\t:\n",
            "  (0, 65798)\t0.08764480627432894\n",
            "  (0, 64095)\t0.048786024742777\n",
            "  (0, 59779)\t0.06825052577984377\n",
            "  (0, 56979)\t0.02638497269006967\n",
            "  (0, 56283)\t0.034656523636315974\n",
            "  (0, 53521)\t0.13145063115405406\n",
            "  (0, 51353)\t0.16598346456058113\n",
            "  (0, 48753)\t0.0874317217290563\n",
            "  (0, 48546)\t0.058499941753033254\n",
            "  (0, 47246)\t0.27461404870175704\n",
            "  (0, 42876)\t0.06820497553514267\n",
            "  (0, 41785)\t0.14568990905086554\n",
            "  (0, 41316)\t0.11537829875562776\n",
            "  (0, 41105)\t0.04905995756763878\n",
            "  (0, 37565)\t0.09453279568632154\n",
            "  (0, 35456)\t0.14656221899872923\n",
            "  (0, 30044)\t0.047602978449929216\n",
            "  (0, 29987)\t0.18992933258978528\n",
            "  (0, 28615)\t0.0707846649625905\n",
            "  (0, 28601)\t0.05295157472547976\n",
            "  (0, 28012)\t0.04971794322459706\n",
            "  (0, 27001)\t0.18992933258978528\n",
            "  (0, 25605)\t0.29219228699617206\n",
            "  (0, 18888)\t0.19639688195070834\n",
            "  (0, 2927)\t0.09661785479371196\n"
          ]
        }
      ],
      "source": [
        "print(X_train_tfidf[0,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JRtlsIlX3KJ"
      },
      "source": [
        "Again we apply the transformation to the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGEiD2tWX3KJ"
      },
      "outputs": [],
      "source": [
        "X_test_tfidf = tfidf_tranformer.transform(X_test_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyveNTbUX3KJ"
      },
      "source": [
        "This should suffice as features to train a classifer (for the moment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ6F8ZM1X3KJ"
      },
      "source": [
        "### Vectorise labels\n",
        "Next, we deal with the labels. Every document has exactly one label attached. We have 20 labels in total. This means we can basically assign a number to each label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhdcRdMOX3KJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yedDDaqeX3KK",
        "outputId": "b637d0bb-44b5-47c1-84bf-c11e6d6c476d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7QJT1iMX3KK",
        "outputId": "872716d7-830a-4609-8d6b-5e6bd68fafd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11314,)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0G3-3BOX3KK",
        "outputId": "d86bc789-a2eb-444e-fe27-0131f3f76228"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7532,)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7QgLsXNX3KK",
        "outputId": "a0d4648d-de74-4b52-fc92-6dc248a62570"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
              "       'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
              "       'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
              "       'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
              "       'sci.electronics', 'sci.med', 'sci.space',\n",
              "       'soc.religion.christian', 'talk.politics.guns',\n",
              "       'talk.politics.mideast', 'talk.politics.misc',\n",
              "       'talk.religion.misc'], dtype=object)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_encoder.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOpydQgDX3KK"
      },
      "source": [
        "## Finally, let's train models\n",
        "Now it's time to train models. Let's stick to the Multinomial Naive Bayes classifier for the moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5E49BmvX3KL",
        "outputId": "3ba527fb-79a0-456b-e67b-a2cef082427c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY8jOFswX3KL"
      },
      "source": [
        "Let's see how well we do on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qSKvif7X3KL",
        "outputId": "d8303229-208d-4e00-ab69-e81988bc4b7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([18,  8, 11, ...,  2,  7, 15])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nb_clf.predict(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veDcE5vrX3KL",
        "outputId": "84a3e57d-30ed-4bb4-8056-44cec0add9fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([18,  8, 12, ...,  2,  9, 13])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSrwY3o8X3KL"
      },
      "source": [
        "Computing the accuracy is simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCBBI7ACX3KM",
        "outputId": "390974e6-a076-4b36-fc97-661b5ce6ce1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7738980350504514\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "\n",
        "for index, prediction in enumerate(nb_clf.predict(X_test_tfidf)):\n",
        "    if prediction == y_test[index]:\n",
        "        correct +=1\n",
        "\n",
        "print('Accuracy: ', correct/y_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwKSxJ4xX3KM",
        "outputId": "85e61f81-d5ba-4755-9cc5-5cb24371cc89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7738980350504514"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(nb_clf.predict(X_test_tfidf), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcTQCrM_X3KM"
      },
      "source": [
        "Almost 80 percent, that is not too bad. What about a Support Vector Classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI33TO6QX3KM",
        "outputId": "4ec23ca3-bc00-4f86-b6be-66d1917621c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearSVC()"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJEbYM4ZX3KM",
        "outputId": "55a9ff20-14e9-475b-8658-f8d112e8f451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8531598513011153\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "\n",
        "for index, prediction in enumerate(svc.predict(X_test_tfidf)):\n",
        "    if prediction == y_test[index]:\n",
        "        correct +=1\n",
        "\n",
        "print('Accuracy: ', correct/y_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxGpuK1pX3KM"
      },
      "source": [
        "An increase of 8%, that's good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qckHMB-X3KM"
      },
      "source": [
        "However, in order to determine the performance of our models we need cross validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIVCpA-rX3KN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(nb_clf, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADvb5_dwX3KN",
        "outputId": "940342a9-193c-48a6-9ef9-107cc540e64b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.84628975, 0.84363958, 0.85159011, 0.84805654, 0.8311229 ,\n",
              "       0.85587975, 0.85499558, 0.8443855 , 0.85411141, 0.85057471])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgM4c9W-X3KN"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(svc, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WanFN3RLX3KN",
        "outputId": "d8dc2eec-52ba-40c5-b35b-7c4c2f2addf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.92226148, 0.93639576, 0.93462898, 0.92932862, 0.9239611 ,\n",
              "       0.93015031, 0.92749779, 0.92307692, 0.93103448, 0.93280283])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1_rQidWX3KN"
      },
      "source": [
        "We can also calculate precision, recall, and f1 relatively easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H04oOtkMX3KN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(max_iter=50)\n",
        "sgd_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_train_predictions = cross_val_predict(sgd_clf, X_train_tfidf, y_train, cv=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUrvEIDKX3KN",
        "outputId": "e13acd88-530b-46bc-b08d-e18d5aeb85a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9180661127806258\n",
            "0.9180661127806258\n",
            "0.9180661127806258\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[443,   1,   0,   2,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          1,   1,   9,   1,   3,   0,  18],\n",
              "       [  0, 503,  23,  15,   5,  20,   6,   1,   0,   0,   1,   0,   3,\n",
              "          3,   4,   0,   0,   0,   0,   0],\n",
              "       [  1,  19, 524,  19,   3,  14,   5,   0,   0,   0,   0,   0,   3,\n",
              "          0,   1,   1,   0,   0,   1,   0],\n",
              "       [  0,  22,  32, 472,  15,   4,  17,   3,   0,   0,   2,   0,  23,\n",
              "          0,   0,   0,   0,   0,   0,   0],\n",
              "       [  1,   7,   7,  20, 513,   3,  10,   0,   4,   0,   0,   1,   7,\n",
              "          2,   1,   1,   0,   0,   1,   0],\n",
              "       [  0,  22,  10,   6,   1, 542,   2,   1,   1,   1,   2,   2,   1,\n",
              "          2,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   2,   3,  15,   4,   1, 518,  13,   5,   0,   5,   2,  11,\n",
              "          1,   1,   2,   0,   1,   1,   0],\n",
              "       [  0,   3,   3,   3,   2,   3,  10, 539,   9,   4,   1,   0,  10,\n",
              "          1,   1,   1,   2,   0,   2,   0],\n",
              "       [  1,   1,   0,   0,   2,   1,  12,   7, 571,   1,   0,   0,   1,\n",
              "          0,   1,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   1,   1,   0,   2,   2,   5,   2, 575,   6,   1,   1,\n",
              "          0,   0,   0,   0,   0,   1,   0],\n",
              "       [  0,   2,   0,   1,   1,   0,   3,   1,   0,   5, 584,   0,   0,\n",
              "          0,   0,   1,   0,   1,   1,   0],\n",
              "       [  1,   5,   1,   1,   0,   2,   0,   0,   0,   1,   2, 569,   4,\n",
              "          1,   0,   2,   1,   1,   4,   0],\n",
              "       [  0,   9,   4,  20,   6,   3,  11,   7,   2,   2,   2,   1, 517,\n",
              "          1,   3,   2,   0,   1,   0,   0],\n",
              "       [  0,   3,   2,   2,   2,   3,   3,   2,   0,   0,   0,   2,   5,\n",
              "        566,   2,   1,   1,   0,   0,   0],\n",
              "       [  0,   5,   0,   1,   1,   0,   3,   0,   0,   0,   0,   0,   1,\n",
              "          1, 577,   0,   1,   1,   2,   0],\n",
              "       [  4,   1,   3,   3,   1,   0,   2,   1,   0,   1,   1,   1,   1,\n",
              "          2,   0, 570,   1,   3,   0,   4],\n",
              "       [  0,   0,   1,   0,   0,   1,   3,   3,   1,   1,   1,   2,   0,\n",
              "          2,   1,   3, 524,   0,   2,   1],\n",
              "       [  1,   0,   0,   0,   0,   0,   1,   1,   0,   1,   0,   0,   0,\n",
              "          1,   0,   3,   0, 555,   1,   0],\n",
              "       [  3,   1,   0,   0,   0,   1,   2,   2,   0,   1,   2,   2,   1,\n",
              "          4,   1,   2,   8,   6, 426,   3],\n",
              "       [ 19,   0,   0,   0,   0,   3,   1,   2,   2,   1,   1,   0,   1,\n",
              "          3,   1,  31,   8,   1,   4, 299]])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(precision_score(y_train, y_train_predictions, average='micro'))\n",
        "print(recall_score(y_train, y_train_predictions, average='micro'))\n",
        "print(f1_score(y_train, y_train_predictions, average='micro'))\n",
        "conf_mx = confusion_matrix(y_train, y_train_predictions)\n",
        "conf_mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzndJUMpX3KO",
        "outputId": "69e76ed5-6414-4f3f-e3cd-d05ad4d7c860"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCMeQ-TZX3KO",
        "outputId": "25153cc5-8424-4b91-a337-966b5d7182ca"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
        "norm_conf_mx = conf_mx / row_sums\n",
        "\n",
        "np.fill_diagonal(norm_conf_mx, 0)\n",
        "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOZGvBUoX3KO",
        "outputId": "e392825b-457d-4fb6-ab6e-e4c0df0cb456",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
              "       'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
              "       'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
              "       'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
              "       'sci.electronics', 'sci.med', 'sci.space',\n",
              "       'soc.religion.christian', 'talk.politics.guns',\n",
              "       'talk.politics.mideast', 'talk.politics.misc',\n",
              "       'talk.religion.misc'], dtype=object)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_encoder.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br-EQ2AoX3KO"
      },
      "source": [
        "## Shortcuts in sklearn - pipelines\n",
        "Sklearn allows us to build convenient `Pipelines`, which facilitate the management of our data and the training of our models enourmously. Consider for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbTKdcoeX3KO"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('nb_clf', MultinomialNB())\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx1_FDKcX3KO"
      },
      "source": [
        "We could even replace the two first lines of the pipeline by using `TfidfVectorizer`, which first fits and transforms the input the same way as the `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCysE7tLX3KO",
        "outputId": "bf2ec1f9-da55-4318-8677-cc42e0dcb9da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('nb_clf', MultinomialNB())])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmX9mh6eX3KP"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(text_clf, X_train, y_train, scoring='accuracy', cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBKGQzJLX3KP",
        "outputId": "6f65070b-5bc8-4040-e662-5d9506f87d82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.84717314, 0.84452297, 0.85424028, 0.85335689, 0.83289125,\n",
              "       0.85676393, 0.85499558, 0.8443855 , 0.85676393, 0.85057471])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWKm1CyaX3KP"
      },
      "source": [
        "## Model selection - find your best model\n",
        "For every model you would like to train, there is a plethora of parameters you could set. How to find the best model? Again, sklearn has a solution: `GridSearchCV`. With grid search cross validation, you can set your hyperparameter space and train different models with all the parameter combinations. Keep in mind that depending on how many folds you train, the whole training procedure takes significantly longer. But let's set up grid search cross validation. We set up a new pipeline for a SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuyNDDbjX3KP",
        "outputId": "7354dd72-b87d-44f3-d909-2a09fcfd7ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  40 out of  40 | elapsed: 28.7min finished\n",
            "/home/janis/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('vect', CountVectorizer()),\n",
              "                                       ('tfidf', TfidfTransformer()),\n",
              "                                       ('svc', LinearSVC())]),\n",
              "             n_jobs=4,\n",
              "             param_grid={'svc__loss': ['hinge', 'squared_hinge'],\n",
              "                         'svc__multi_class': ['ovr', 'crammer_singer'],\n",
              "                         'vect__ngram_range': [(1, 1), (1, 2)]},\n",
              "             verbose=1)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "text_svc = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('svc', LinearSVC())\n",
        "])\n",
        "\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "             'svc__loss': ['hinge', 'squared_hinge'],\n",
        "             'svc__multi_class': ['ovr', 'crammer_singer']}\n",
        "\n",
        "gs_svc = GridSearchCV(text_svc, param_grid, cv=5, n_jobs=4, verbose=1)\n",
        "gs_svc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyKZRLITX3KP",
        "outputId": "8ec7e7ff-5d40-4543-ef9b-a3c4adf412dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_svc__loss</th>\n",
              "      <th>param_svc__multi_class</th>\n",
              "      <th>param_vect__ngram_range</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>436.592107</td>\n",
              "      <td>171.236469</td>\n",
              "      <td>2.384123</td>\n",
              "      <td>0.070692</td>\n",
              "      <td>hinge</td>\n",
              "      <td>crammer_singer</td>\n",
              "      <td>(1, 2)</td>\n",
              "      <td>{'svc__loss': 'hinge', 'svc__multi_class': 'cr...</td>\n",
              "      <td>0.933716</td>\n",
              "      <td>0.935484</td>\n",
              "      <td>0.933274</td>\n",
              "      <td>0.925320</td>\n",
              "      <td>0.930592</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.003544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>433.028093</td>\n",
              "      <td>113.623423</td>\n",
              "      <td>2.362510</td>\n",
              "      <td>0.188189</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>crammer_singer</td>\n",
              "      <td>(1, 2)</td>\n",
              "      <td>{'svc__loss': 'squared_hinge', 'svc__multi_cla...</td>\n",
              "      <td>0.933716</td>\n",
              "      <td>0.935484</td>\n",
              "      <td>0.933274</td>\n",
              "      <td>0.925320</td>\n",
              "      <td>0.930592</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.003544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>35.854953</td>\n",
              "      <td>1.288269</td>\n",
              "      <td>2.438577</td>\n",
              "      <td>0.162266</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>ovr</td>\n",
              "      <td>(1, 2)</td>\n",
              "      <td>{'svc__loss': 'squared_hinge', 'svc__multi_cla...</td>\n",
              "      <td>0.929739</td>\n",
              "      <td>0.931507</td>\n",
              "      <td>0.931065</td>\n",
              "      <td>0.924437</td>\n",
              "      <td>0.930592</td>\n",
              "      <td>0.929468</td>\n",
              "      <td>0.002583</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>176.695410</td>\n",
              "      <td>15.243509</td>\n",
              "      <td>2.438571</td>\n",
              "      <td>0.082104</td>\n",
              "      <td>hinge</td>\n",
              "      <td>ovr</td>\n",
              "      <td>(1, 2)</td>\n",
              "      <td>{'svc__loss': 'hinge', 'svc__multi_class': 'ov...</td>\n",
              "      <td>0.929739</td>\n",
              "      <td>0.932391</td>\n",
              "      <td>0.930623</td>\n",
              "      <td>0.922669</td>\n",
              "      <td>0.929266</td>\n",
              "      <td>0.928938</td>\n",
              "      <td>0.003311</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9.122730</td>\n",
              "      <td>0.169448</td>\n",
              "      <td>1.026385</td>\n",
              "      <td>0.044109</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>ovr</td>\n",
              "      <td>(1, 1)</td>\n",
              "      <td>{'svc__loss': 'squared_hinge', 'svc__multi_cla...</td>\n",
              "      <td>0.926204</td>\n",
              "      <td>0.931507</td>\n",
              "      <td>0.929739</td>\n",
              "      <td>0.922227</td>\n",
              "      <td>0.925729</td>\n",
              "      <td>0.927081</td>\n",
              "      <td>0.003250</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>107.631349</td>\n",
              "      <td>46.304807</td>\n",
              "      <td>1.015226</td>\n",
              "      <td>0.028500</td>\n",
              "      <td>hinge</td>\n",
              "      <td>crammer_singer</td>\n",
              "      <td>(1, 1)</td>\n",
              "      <td>{'svc__loss': 'hinge', 'svc__multi_class': 'cr...</td>\n",
              "      <td>0.925762</td>\n",
              "      <td>0.928414</td>\n",
              "      <td>0.927972</td>\n",
              "      <td>0.920460</td>\n",
              "      <td>0.927940</td>\n",
              "      <td>0.926109</td>\n",
              "      <td>0.002972</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>118.908495</td>\n",
              "      <td>45.469967</td>\n",
              "      <td>0.999624</td>\n",
              "      <td>0.033259</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>crammer_singer</td>\n",
              "      <td>(1, 1)</td>\n",
              "      <td>{'svc__loss': 'squared_hinge', 'svc__multi_cla...</td>\n",
              "      <td>0.925762</td>\n",
              "      <td>0.928414</td>\n",
              "      <td>0.927972</td>\n",
              "      <td>0.920460</td>\n",
              "      <td>0.927940</td>\n",
              "      <td>0.926109</td>\n",
              "      <td>0.002972</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23.560421</td>\n",
              "      <td>1.666786</td>\n",
              "      <td>1.010844</td>\n",
              "      <td>0.064981</td>\n",
              "      <td>hinge</td>\n",
              "      <td>ovr</td>\n",
              "      <td>(1, 1)</td>\n",
              "      <td>{'svc__loss': 'hinge', 'svc__multi_class': 'ov...</td>\n",
              "      <td>0.927088</td>\n",
              "      <td>0.930181</td>\n",
              "      <td>0.926204</td>\n",
              "      <td>0.918250</td>\n",
              "      <td>0.924845</td>\n",
              "      <td>0.925314</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
              "3     436.592107    171.236469         2.384123        0.070692   \n",
              "7     433.028093    113.623423         2.362510        0.188189   \n",
              "5      35.854953      1.288269         2.438577        0.162266   \n",
              "1     176.695410     15.243509         2.438571        0.082104   \n",
              "4       9.122730      0.169448         1.026385        0.044109   \n",
              "2     107.631349     46.304807         1.015226        0.028500   \n",
              "6     118.908495     45.469967         0.999624        0.033259   \n",
              "0      23.560421      1.666786         1.010844        0.064981   \n",
              "\n",
              "  param_svc__loss param_svc__multi_class param_vect__ngram_range  \\\n",
              "3           hinge         crammer_singer                  (1, 2)   \n",
              "7   squared_hinge         crammer_singer                  (1, 2)   \n",
              "5   squared_hinge                    ovr                  (1, 2)   \n",
              "1           hinge                    ovr                  (1, 2)   \n",
              "4   squared_hinge                    ovr                  (1, 1)   \n",
              "2           hinge         crammer_singer                  (1, 1)   \n",
              "6   squared_hinge         crammer_singer                  (1, 1)   \n",
              "0           hinge                    ovr                  (1, 1)   \n",
              "\n",
              "                                              params  split0_test_score  \\\n",
              "3  {'svc__loss': 'hinge', 'svc__multi_class': 'cr...           0.933716   \n",
              "7  {'svc__loss': 'squared_hinge', 'svc__multi_cla...           0.933716   \n",
              "5  {'svc__loss': 'squared_hinge', 'svc__multi_cla...           0.929739   \n",
              "1  {'svc__loss': 'hinge', 'svc__multi_class': 'ov...           0.929739   \n",
              "4  {'svc__loss': 'squared_hinge', 'svc__multi_cla...           0.926204   \n",
              "2  {'svc__loss': 'hinge', 'svc__multi_class': 'cr...           0.925762   \n",
              "6  {'svc__loss': 'squared_hinge', 'svc__multi_cla...           0.925762   \n",
              "0  {'svc__loss': 'hinge', 'svc__multi_class': 'ov...           0.927088   \n",
              "\n",
              "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
              "3           0.935484           0.933274           0.925320           0.930592   \n",
              "7           0.935484           0.933274           0.925320           0.930592   \n",
              "5           0.931507           0.931065           0.924437           0.930592   \n",
              "1           0.932391           0.930623           0.922669           0.929266   \n",
              "4           0.931507           0.929739           0.922227           0.925729   \n",
              "2           0.928414           0.927972           0.920460           0.927940   \n",
              "6           0.928414           0.927972           0.920460           0.927940   \n",
              "0           0.930181           0.926204           0.918250           0.924845   \n",
              "\n",
              "   mean_test_score  std_test_score  rank_test_score  \n",
              "3         0.931677        0.003544                1  \n",
              "7         0.931677        0.003544                1  \n",
              "5         0.929468        0.002583                3  \n",
              "1         0.928938        0.003311                4  \n",
              "4         0.927081        0.003250                5  \n",
              "2         0.926109        0.002972                6  \n",
              "6         0.926109        0.002972                6  \n",
              "0         0.925314        0.003943                8  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svc_df = pd.DataFrame.from_dict(gs_svc.cv_results_)\n",
        "svc_df.sort_values(by=[\"rank_test_score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2OuEAVLX3KP",
        "outputId": "a155e9bb-8c25-4595-c9c3-69a0fba96c51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([18,  8, 12, ...,  2,  6, 13])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gs_svc.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-HQRx4kX3KQ",
        "outputId": "d328a0b5-54c7-4679-ac03-62ecefbc0c86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([18,  8, 12, ...,  2,  9, 13])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyPQWIOzX3KQ",
        "outputId": "99452009-e19a-4a3c-f58c-88a07e2f2cf6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/janis/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
              "                ('tfidf', TfidfTransformer()),\n",
              "                ('svc', LinearSVC(loss='hinge', multi_class='crammer_singer'))])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('svc', LinearSVC(loss='hinge', multi_class='crammer_singer'))\n",
        "])\n",
        "\n",
        "best_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIrLAwyyX3KQ",
        "outputId": "0e682ff7-d0b0-4154-a8c7-d8fa891cd564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([18,  8, 12, ...,  2,  6, 13])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCF056HrX3KQ",
        "outputId": "5febc2b8-c4da-4c24-c65a-16609fa042de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.8600637280934679\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "\n",
        "for index, prediction in enumerate(best_model.predict(X_test)):\n",
        "    if prediction == y_test[index]:\n",
        "        correct +=1\n",
        "\n",
        "print('Accuracy: ', correct/y_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBLEzh-MCVfe"
      },
      "source": [
        "##  (NOT CURRENT CONTENT) Modern Solutions Sneak Peek - Transformer\n",
        "\n",
        "Let's look at another task, paraphrase detection. Do two sentences have the same meaning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwhWBd2aEqeF",
        "outputId": "32b3ee12-ed1a-4151-93cd-1bdf2f771f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGNDxHAlEHY0",
        "outputId": "f44fc39a-5549-4a59-fb7f-3f75e7e0b971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1\n",
            "Sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\n",
            "Sentence 2: \" The foodservice pie business does not fit our long-term growth strategy .\n",
            "Prediction: Paraphrase\n",
            "Ground Truth: Paraphrase\n",
            "==================================================\n",
            "Example 2\n",
            "Sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war .\n",
            "Sentence 2: His wife said he was \" 100 percent behind George Bush \" and looked forward to using his years of training in the war .\n",
            "Prediction: Not a Paraphrase\n",
            "Ground Truth: Not a Paraphrase\n",
            "==================================================\n",
            "Example 3\n",
            "Sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat .\n",
            "Sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .\n",
            "Prediction: Not a Paraphrase\n",
            "Ground Truth: Not a Paraphrase\n",
            "==================================================\n",
            "Example 4\n",
            "Sentence 1: The AFL-CIO is waiting until October to decide if it will endorse a candidate .\n",
            "Sentence 2: The AFL-CIO announced Wednesday that it will decide in October whether to endorse a candidate before the primaries .\n",
            "Prediction: Not a Paraphrase\n",
            "Ground Truth: Paraphrase\n",
            "==================================================\n",
            "Example 5\n",
            "Sentence 1: No dates have been set for the civil or the criminal trial .\n",
            "Sentence 2: No dates have been set for the criminal or civil cases , but Shanley has pleaded not guilty .\n",
            "Prediction: Not a Paraphrase\n",
            "Ground Truth: Not a Paraphrase\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# 1. Load Pre-trained Model and Tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"AMHR/adversarial-paraphrasing-detector\"  # Replace with the model you want to use\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 2. Prepare the Dataset\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "eval_dataset = dataset[\"validation\"]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# 3. Evaluate the Model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, example in enumerate(eval_dataset):\n",
        "        if i > 5:\n",
        "           break\n",
        "        # Tokenize the inputs and get the model's predictions\n",
        "        inputs = tokenizer(example['sentence1'], example['sentence2'], return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "        # Move input tensors to the same device as the model\n",
        "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        # Print first 5 example pairs along with predictions and ground truth labels\n",
        "        if i < 5:\n",
        "            print(f\"Example {i+1}\")\n",
        "            print(f\"Sentence 1: {example['sentence1']}\")\n",
        "            print(f\"Sentence 2: {example['sentence2']}\")\n",
        "            print(f\"Prediction: {'Paraphrase' if predictions == 1 else 'Not a Paraphrase'}\")\n",
        "            print(f\"Ground Truth: {'Paraphrase' if example['label'] == 1 else 'Not a Paraphrase'}\")\n",
        "            print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh0ryN2WG-dM"
      },
      "outputs": [],
      "source": [
        "def predict_paraphrase(sentence1, sentence2, model, tokenizer, device):\n",
        "    # Prepare the sentences for the model\n",
        "    inputs = tokenizer(sentence1, sentence2, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # Move the input tensors to the device the model is on\n",
        "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get model's prediction\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    return \"Paraphrase\" if prediction == 1 else \"Not a Paraphrase\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpuHVRgOGvxL",
        "outputId": "b07a703c-9f19-403c-8404-55035cabe948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: This tutorial rocks.\n",
            "Sentence 2: I want to throw rocks at this Tutor.\n",
            "Prediction: Not a Paraphrase\n"
          ]
        }
      ],
      "source": [
        "# Custom Sentences testing\n",
        "sentence1 = \"This tutorial rocks.\"\n",
        "sentence2 = \"I want to throw rocks at this Tutor.\"\n",
        "\n",
        "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
        "print(f\"Sentence 1: {sentence1}\")\n",
        "print(f\"Sentence 2: {sentence2}\")\n",
        "print(f\"Prediction: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RibEJxxdICU7",
        "outputId": "62756b05-e77a-4220-c7d4-899f5d6605c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: I am so tired but I want to stay in this tutorial.\n",
            "Sentence 2: I am exhausted and forced to be here.\n",
            "Prediction: Not a Paraphrase\n"
          ]
        }
      ],
      "source": [
        "# Custom Sentences Testing\n",
        "sentence1 = \"I am so tired but I want to stay in this tutorial.\"\n",
        "sentence2 = \"I am exhausted and forced to be here.\"\n",
        "\n",
        "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
        "print(f\"Sentence 1: {sentence1}\")\n",
        "print(f\"Sentence 2: {sentence2}\")\n",
        "print(f\"Prediction: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6HGgabNIcsy",
        "outputId": "4f4310d1-00b5-4948-e0c1-4716c44cf156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: This field of research is pretty cool.\n",
            "Sentence 2: I find this line of research very cool.\n",
            "Prediction: Paraphrase\n"
          ]
        }
      ],
      "source": [
        "# Custom Sentences Testing\n",
        "sentence1 = \"This field of research is pretty cool.\"\n",
        "sentence2 = \"I find this line of research very cool.\"\n",
        "\n",
        "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
        "print(f\"Sentence 1: {sentence1}\")\n",
        "print(f\"Sentence 2: {sentence2}\")\n",
        "print(f\"Prediction: {result}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MZ6F8ZM1X3KJ",
        "vOpydQgDX3KK"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
