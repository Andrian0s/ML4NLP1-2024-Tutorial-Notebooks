{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5QIfalB5dkxUOhI7TqmyB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/tutorials_notebooks_in_class_2024/w11_picoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is an adaptation of [Jay Mody's blog post on picoGPT](https://jaykmody.com/blog/gpt-from-scratch/).\n",
        "\n",
        "The goal of this notebook is to demonstrate how GPT works with minimal and basic numpy code.\n",
        "\n",
        "\n",
        "Note, as a complement to this resource, we recommend reading the [Illustrated GPT blog by Jay Alammar](https://jalammar.github.io/illustrated-gpt2/), which provides a very nice and visual description of the internal processes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_z11MAxvBvA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gI4_chOkkN_Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/jaymody/picoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r 'picoGPT/requirements.txt'"
      ],
      "metadata": {
        "id": "q6wKapRbkTtT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icecream"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpb1yJvx_u1A",
        "outputId": "a5184aaf-2017-4172-ded6-1f9b4375840d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting icecream\n",
            "  Downloading icecream-2.1.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting colorama>=0.3.9 (from icecream)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.18.0)\n",
            "Collecting executing>=0.3.1 (from icecream)\n",
            "  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
            "Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: executing, colorama, asttokens, icecream\n",
            "Successfully installed asttokens-2.4.1 colorama-0.4.6 executing-2.1.0 icecream-2.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the commandline call to make sure installation worked\n",
        "# !python picoGPT/gpt2.py \"Alan Turing theorized that computers would one day become\""
      ],
      "metadata": {
        "id": "mEojLAvzkpQc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import sys\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from icecream import ic\n",
        "\n",
        "# Add the picoGPT directory to sys.path in order to import helpers\n",
        "if not '/content/picoGPT' in sys.path:\n",
        "    sys.path.append('/content/picoGPT')\n",
        "    print(sys.path)\n",
        "\n",
        "from utils import load_encoder_hparams_and_params\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFQYeRi29pSG",
        "outputId": "49c5507a-8cc4-4ddb-bfb8-7e0339129dea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython', '/content/picoGPT']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting the next token and computing the loss for the sequence:\n",
        "\n",
        "`Not all heroes wear capes`"
      ],
      "metadata": {
        "id": "CobBQ7AdBzWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['not', 'all', 'heroes', 'wear', 'capes'] # 5 items in our vocabulary\n",
        "\n",
        "def dummy_gpt(x: np.array, stochastic: bool =False):\n",
        "    \"\"\"\n",
        "    A 'dummy' GPT. Takes an input x sequence and outputs a matrix with next token prediction probabilities for a given vocab.\n",
        "\n",
        "    Args:\n",
        "        x: input numpy array\n",
        "        stochastic: if True, the output matrix is randomly generated.\n",
        "    Returns:\n",
        "        output matrix contains all the predicted next token probability distributions at each position and has shape [num_tokens_in_input_seq - 1, num_tokens_in_vocab]\n",
        "    \"\"\"\n",
        "    if stochastic:\n",
        "        output = np.random.rand(len(x), len(vocab)) # random values\n",
        "        output /= output.sum(axis=1, keepdims=True) # normalize to [0,1]\n",
        "    else:\n",
        "        output = np.array([\n",
        "            [0.1, 0.4, 0.2, 0.1, 0.2], # Probabilities for predicting the next token after \"not\"\n",
        "            [0.05, 0.1, 0.7, 0.1, 0.05], # Probabilities for predicting the next token after \"all\"\n",
        "            [0.1, 0.1, 0.1, 0.6, 0.1], # Probabilities for predicting the next token after \"heroes\"\n",
        "            [0.2, 0.2, 0.1, 0.1, 0.4], # Probabilities for predicting the next token after \"wear\"\n",
        "        ])\n",
        "    return output\n",
        "\n",
        "inputs = np.array([0, 1, 2, 3, 4]) #\n",
        "x = np.array([0, 1, 2, 3]) # [not, all, heroes, wear]\n",
        "y = np.array([1, 2, 3, 4]) # [all, heroes, wear, capes]\n",
        "\n",
        "ic([vocab[i] for i in inputs])\n",
        "ic([vocab[i] for i in x])\n",
        "ic([vocab[i] for i in y])\n",
        "\n",
        "# What is the loss when the prediction probabilities are random?\n",
        "output = dummy_gpt(x, stochastic=True)\n",
        "ic(output)\n",
        "\n",
        "o = output[np.arange(len(output)), y]\n",
        "ic(o)\n",
        "\n",
        "loss = np.mean(-np.log(o))\n",
        "ic(loss)"
      ],
      "metadata": {
        "id": "EH_vNMbvlFGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15fe2d0e-1710-47d3-80a9-8cc9990e9598"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| [vocab[i] for i in inputs]: ['not', 'all', 'heroes', 'wear', 'capes']\n",
            "ic| [vocab[i] for i in x]: ['not', 'all', 'heroes', 'wear']\n",
            "ic| [vocab[i] for i in y]: ['all', 'heroes', 'wear', 'capes']\n",
            "ic| output: array([[0.05730092, 0.29125855, 0.38442879, 0.05596454, 0.2110472 ],\n",
            "                   [0.12707577, 0.13135543, 0.20415093, 0.35553657, 0.1818813 ],\n",
            "                   [0.07241317, 0.1042076 , 0.38438858, 0.11355035, 0.3254403 ],\n",
            "                   [0.11457584, 0.18748023, 0.20604333, 0.21017845, 0.28172215]])\n",
            "ic| o: array([0.29125855, 0.20415093, 0.11355035, 0.28172215])\n",
            "ic| loss: 1.5661956314610228\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5661956314610228"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare above with what happens when the prediction probabilities are closer to the ground truth:\n",
        "output = dummy_gpt(x, stochastic=False)\n",
        "ic(output)\n",
        "\n",
        "o = output[np.arange(len(output)), y]\n",
        "ic(o)\n",
        "\n",
        "loss = np.mean(-np.log(o))\n",
        "ic(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keKVognin6tf",
        "outputId": "da9d1532-9260-4b90-f0fd-a89d5ca0a91b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| output: array([[0.1 , 0.4 , 0.2 , 0.1 , 0.2 ],\n",
            "                   [0.05, 0.1 , 0.7 , 0.1 , 0.05],\n",
            "                   [0.1 , 0.1 , 0.1 , 0.6 , 0.1 ],\n",
            "                   [0.2 , 0.2 , 0.1 , 0.1 , 0.4 ]])\n",
            "ic| o: array([0.4, 0.7, 0.6, 0.4])\n",
            "ic| loss: 0.6750205078632583\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6750205078632583"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the tokenizer"
      ],
      "metadata": {
        "id": "J5jlc_UmCNRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer, hparams, and params from the released open-ai gpt-2 files\n",
        "model_size: str = \"124M\"\n",
        "models_dir: str = \"models\"\n",
        "tokenizer, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
        "\n",
        "# set the number of tokens to generate\n",
        "n_tokens_to_generate: int = 40"
      ],
      "metadata": {
        "id": "jbRC3U3TtaZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a0d705-1f14-49e8-bae8-ca78032829d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.00kb [00:00, 4.17Mb/s]                                                       \n",
            "Fetching encoder.json: 1.04Mb [00:00, 2.93Mb/s]                                                     \n",
            "Fetching hparams.json: 1.00kb [00:00, 1.26Mb/s]                                                     \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mb [00:12, 38.9Mb/s]                                    \n",
            "Fetching model.ckpt.index: 6.00kb [00:00, 13.1Mb/s]                                                 \n",
            "Fetching model.ckpt.meta: 472kb [00:00, 1.73Mb/s]                                                   \n",
            "Fetching vocab.bpe: 457kb [00:00, 1.94Mb/s]                                                         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Not all heroes wear capes.\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "\n",
        "print(f'\\nToken IDs for the sequence `{prompt}`:\\n')\n",
        "print(input_ids)\n",
        "\n",
        "print(f'\\nTokens for the sequence `{prompt}`:\\n')\n",
        "print([tokenizer.decoder[i] for i in input_ids])\n",
        "\n",
        "# make sure we are not surpassing the max sequence length of our model\n",
        "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD-FZkgBsPca",
        "outputId": "5c660545-ef06-4755-cf29-3eadec97be06"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token IDs for the sequence `Not all heroes wear capes.`:\n",
            "\n",
            "[3673, 477, 10281, 5806, 1451, 274, 13]\n",
            "\n",
            "Tokens for the sequence `Not all heroes wear capes.`:\n",
            "\n",
            "['Not', 'Ġall', 'Ġheroes', 'Ġwear', 'Ġcap', 'es', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> What is the significance of `Ġ`?\n"
      ],
      "metadata": {
        "id": "4bTUNACrDTpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect model parameters"
      ],
      "metadata": {
        "id": "PLXHiC3cDi-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params is a nested json dictionary that holds the trained weights of our model. The leaf nodes of the json are NumPy arrays\n",
        "\n",
        "# {\n",
        "#     \"wpe\": [n_ctx, n_embd],\n",
        "#     \"wte\": [n_vocab, n_embd],\n",
        "#     \"ln_f\": {\"b\": [n_embd], \"g\": [n_embd]},\n",
        "#     \"blocks\": [\n",
        "#         {\n",
        "#             \"attn\": {\n",
        "#                 \"c_attn\": {\"b\": [3*n_embd], \"w\": [n_embd, 3*n_embd]},\n",
        "#                 \"c_proj\": {\"b\": [n_embd], \"w\": [n_embd, n_embd]},\n",
        "#             },\n",
        "#             \"ln_1\": {\"b\": [n_embd], \"g\": [n_embd]},\n",
        "#             \"ln_2\": {\"b\": [n_embd], \"g\": [n_embd]},\n",
        "#             \"mlp\": {\n",
        "#                 \"c_fc\": {\"b\": [4*n_embd], \"w\": [n_embd, 4*n_embd]},\n",
        "#                 \"c_proj\": {\"b\": [n_embd], \"w\": [4*n_embd, n_embd]},\n",
        "#             },\n",
        "#         },\n",
        "#         ... # repeat for n_layers\n",
        "#     ]\n",
        "# }"
      ],
      "metadata": {
        "id": "Vvam5cNc8m6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shape_tree(d):\n",
        "    \"\"\"\n",
        "    Helper function to print shapes of objects in a nested dictionary.\n",
        "    \"\"\"\n",
        "    if isinstance(d, np.ndarray):\n",
        "        return list(d.shape)\n",
        "    elif isinstance(d, list):\n",
        "        return [shape_tree(v) for v in d]\n",
        "    elif isinstance(d, dict):\n",
        "        return {k: shape_tree(v) for k, v in d.items()}\n",
        "    else:\n",
        "        ValueError(\"uh oh\")\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "pprint(type(params))\n",
        "pprint(params.keys())\n",
        "pprint(shape_tree(params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_9cO5VX82OA",
        "outputId": "99880ae7-97cf-4951-8ccf-c8ebf90aa7f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "dict_keys(['blocks', 'ln_f', 'wpe', 'wte'])\n",
            "{'blocks': [{'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
            "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
            "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
            "             'ln_1': {'b': [768], 'g': [768]},\n",
            "             'ln_2': {'b': [768], 'g': [768]},\n",
            "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
            "                     'c_proj': {'b': [768], 'w': [3072, 768]}}}],\n",
            " 'ln_f': {'b': [768], 'g': [768]},\n",
            " 'wpe': [1024, 768],\n",
            " 'wte': [50257, 768]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> What is the model's hidden dimension?\n",
        "\n",
        "> How deep is the model (i.e. how many transformer blocks)?\n",
        "\n",
        "> What is the maximum sequence length?\n",
        "\n",
        "> How many tokens are in the vocabulary?\n",
        "\n",
        "> How many params are in each block?\n",
        "\n",
        "> How many params in total?\n",
        "\n",
        "> Compare the attention mechanisms vs. feedforward layers vs. embedding layers. Where are the majority of the params?"
      ],
      "metadata": {
        "id": "XdD0zoxpEztr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_param_count(d):\n",
        "    \"\"\"\n",
        "    Helper function to print the sum of parameters in a nested dictionary representing a model's parameters\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "\n",
        "    if isinstance(d, np.ndarray):\n",
        "        return np.prod(d.shape)\n",
        "\n",
        "    elif isinstance(d, dict):\n",
        "        for value in d.values():\n",
        "            total += get_param_count(value)\n",
        "\n",
        "    elif isinstance(d, (list, tuple)):\n",
        "        for item in d:\n",
        "            total += get_param_count(item)\n",
        "\n",
        "    return total\n",
        "\n",
        "total_c = get_param_count(params)\n",
        "attn_param_c = sum([get_param_count(block['attn']) for block in params['blocks']])\n",
        "ff_param_c = sum([get_param_count(block['mlp']) for block in params['blocks']])\n",
        "emb_param_c = get_param_count(params['wte'])\n",
        "\n",
        "print(f'Total params: {total_c}')\n",
        "print(f'Attention params: {attn_param_c}, {attn_param_c/total_c*100:.2f}%')\n",
        "print(f'Feedforward params: {ff_param_c}, {ff_param_c/total_c*100:.2f}%')\n",
        "print(f'Embedding params: {emb_param_c}, {emb_param_c/total_c*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGW9aOR5mj2l",
        "outputId": "ac945265-ed7a-40e0-ffda-f9c6cde2ebc7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 124439808\n",
            "Attention params: 28348416, 22.78%\n",
            "Feedforward params: 56669184, 45.54%\n",
            "Embedding params: 38597376, 31.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect hyperparameters (hparams)"
      ],
      "metadata": {
        "id": "x-LHTVAAGvJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hparams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIfOVVOP9ktj",
        "outputId": "d4476807-9406-4e8d-ef32-79d49dc9863c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's code GPT-2"
      ],
      "metadata": {
        "id": "wvIGCXZHG10P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "16ODNUV6n2fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    Activation function used in GPT2\n",
        "\n",
        "    Args:\n",
        "        x: input array\n",
        "    Returns:\n",
        "        array after applying the gelu activation function\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Softmax is used to a convert set of real numbers (between -inf and inf) to probabilities (between 0 and 1, with the numbers all summing to 1)\n",
        "\n",
        "    Args:\n",
        "        x: input array\n",
        "    Returns:\n",
        "        array of probabilities\n",
        "    \"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    \"\"\"\n",
        "    layernorm standardizes values to have a mean of 0 and a variance of 1\n",
        "    it ensures that the inputs for each layer are always within a consistent range, which is supposed to speed up and stabilize the training process\n",
        "    layer normalization over the last axis of the input.\n",
        "\n",
        "    Args:\n",
        "        x: input\n",
        "        g: gamma, corresponds to `g` in ln_1 and ln_2\n",
        "        b: beta, corresponds to `b` in ln_1 and ln_2\n",
        "        eps: epsilon, hyperparameter used to prevent division by zero\n",
        "    Returns:\n",
        "        normalised and rescaled x\n",
        "    \"\"\"\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
        "    return g * x + b  # scale and offset with gamma/beta params\n",
        "\n",
        "\n",
        "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
        "    \"\"\"\n",
        "    Standard matrix multiplication + bias\n",
        "\n",
        "    Args:\n",
        "        x: input array [m, in]\n",
        "        w: weight matrix [in, out]\n",
        "        b: bias vector [out]\n",
        "    Returns:\n",
        "        output array [m, out]\n",
        "    \"\"\"\n",
        "    # the @ operator is syntactic sugar for np.matmul()\n",
        "    return x @ w + b\n",
        "\n",
        "\n",
        "def ffn(x, c_fc, c_proj):\n",
        "    \"\"\"\n",
        "    A simple multi-layer perceptron with 2 layers that projects from n_embd up to a higher dimension 4*n_embd and then back down to n_embd.\n",
        "\n",
        "    Args:\n",
        "        x: input array [n_seq, n_embd]\n",
        "        c_fc: weight matrix for the first layer (projects up) [n_embd, 4*n_embd]\n",
        "        c_proj: weight matrix for the second layer (projects down) [4*n_embd, n_embd]\n",
        "    Returns:\n",
        "        output array [n_seq, n_embd]\n",
        "    \"\"\"\n",
        "    # project up\n",
        "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
        "\n",
        "    # project back down\n",
        "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Computes masked attention given q,k,v matrices\n",
        "\n",
        "    Args:\n",
        "        q: query matrix [n_q, d_k]\n",
        "        k: key matrix [n_k, d_k]\n",
        "        v: value matrix [n_k, d_v]\n",
        "        mask: mask matrix [n_q, n_k]\n",
        "    Returns:\n",
        "        output array [n_q, d_v]\n",
        "    \"\"\"\n",
        "\n",
        "    # print(f'q: {q.shape}, k: {k.shape}, v: {v.shape}')\n",
        "\n",
        "    output = softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    \"\"\"\n",
        "    Multi-head attention\n",
        "\n",
        "    Args:\n",
        "        x: input array [n_seq, n_embd]\n",
        "        c_attn: weight matrix for the attention projection to get q,k,v\n",
        "        c_proj: weight matrix for the projection\n",
        "        n_head: number of heads (usually 12)\n",
        "    Returns:\n",
        "        output array [n_seq, n_embd]\n",
        "    \"\"\"\n",
        "    # qkv projection\n",
        "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
        "\n",
        "    # split into qkv\n",
        "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
        "\n",
        "    # split into heads\n",
        "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # causal mask to hide future inputs from being attended to\n",
        "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
        "\n",
        "    # optional, step through the code to inspect intermediary results\n",
        "    # breakpoint()\n",
        "\n",
        "    # perform attention over each head\n",
        "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
        "\n",
        "    # merge heads\n",
        "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
        "\n",
        "    # out projection\n",
        "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head): # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    \"\"\"\n",
        "    Transformer block with multi-head attention and feed-forward network\n",
        "\n",
        "    Args:\n",
        "        x: input array [n_seq, n_embd]\n",
        "        mlp: weight matrices for the feed-forward network\n",
        "        attn: weight matrices for the multi-head attention\n",
        "        ln_1: weight matrices for the first layer normalization\n",
        "        ln_2: weight matrices for the second layer normalization\n",
        "        n_head: number of heads (usually 12)\n",
        "    Returns:\n",
        "        output array [n_seq, n_embd]\n",
        "    \"\"\"\n",
        "    # multi-head causal self attention\n",
        "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # position-wise feed forward network\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head): # [n_seq] -> [n_seq, n_vocab]\n",
        "    \"\"\"\n",
        "    The real GPT function!\n",
        "\n",
        "    Args:\n",
        "        inputs: input sequence [n_seq]\n",
        "        wte: token embedding matrix\n",
        "        wpe: positional embedding matrix\n",
        "        blocks: list of transformer blocks\n",
        "        ln_f: weight matrices for the final layer normalization\n",
        "        n_head: number of heads (usually 12)\n",
        "    Returns:\n",
        "        output array [n_seq, n_vocab]\n",
        "    \"\"\"\n",
        "\n",
        "    # token + positional embeddings\n",
        "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
        "\n",
        "    # forward pass through n_layer transformer blocks\n",
        "    for block in blocks:\n",
        "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # final layer normalization before projection (special in GPT-2)\n",
        "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    # projection to vocab (note, we are reusing the embedding matrix wte)\n",
        "    logits = x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
        "    \"\"\"\n",
        "    Autorgressive generation function\n",
        "\n",
        "    Args:\n",
        "        inputs: input sequence [n_seq]\n",
        "        params: model parameters\n",
        "        n_head: number of heads (usually 12)\n",
        "        n_tokens_to_generate: number of tokens to generate\n",
        "    \"\"\"\n",
        "\n",
        "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
        "\n",
        "        next_id = np.argmax(logits[-1]) # Note, there is no softmax happening here. Why not?\n",
        "\n",
        "        inputs.append(int(next_id))  # append prediction to input. Why?\n",
        "\n",
        "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids\n",
        "\n",
        "# let's generate!\n",
        "prompt = \"Not all heroes wear capes.\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "\n",
        "n_tokens_to_generate: int = 40\n",
        "\n",
        "# make sure we are not surpassing the max sequence length of our model\n",
        "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
        "\n",
        "# generate output ids\n",
        "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
        "\n",
        "# decode the ids back into a string\n",
        "output_text = tokenizer.decode(output_ids)\n",
        "\n",
        "print('\\n\\nGenerated sequence:\\n')\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzK43NjF1IKr",
        "outputId": "4ce93bfc-d80d-44d4-8859-8dfd801ab3b7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating: 100%|██████████| 40/40 [00:20<00:00,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Generated sequence:\n",
            "\n",
            " Some are more than just a costume.\n",
            "\n",
            "The most common costume is a cape. It's a piece of clothing that's worn by heroes and villains. It's usually a cape that's worn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> In `generate()`, we append the `next_id` to the input sequence rather than cronstructing a new `output` variable to return. Why?\n",
        "\n",
        "> Why do we not need `softmax` in this `generate` function?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2qGj6Dpty65D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the causal mask\n",
        "\n",
        "Let's unpack the line:\n",
        "\n",
        "`causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10`\n"
      ],
      "metadata": {
        "id": "z6Q0G_ZoTQM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: initialise a diagonal array where lower triangle is filled with ones and zero elsewhere\n",
        "m = np.tri(x.shape[0], dtype=x.dtype)\n",
        "print(f'Step 1:\\n{m}')\n",
        "# Step 2: invert so lower triangle is filled with zeros and ones elsewhere\n",
        "m = (1 - m)\n",
        "# Step 3: multiply by -inf to mask indices corresponding to future tokens\n",
        "print(f'Step 2:\\n{m}')\n",
        "m = m * -1e10\n",
        "print(f'Step 3:\\n{m}')\n",
        "\n",
        "# Note, -1e10 = -10000000000.0 approximates -inf after softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrbFshBW65Y9",
        "outputId": "49889938-bb9e-4246-890c-44b9e3da0877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "[[1 0 0 0]\n",
            " [1 1 0 0]\n",
            " [1 1 1 0]\n",
            " [1 1 1 1]]\n",
            "Step 2:\n",
            "[[0 1 1 1]\n",
            " [0 0 1 1]\n",
            " [0 0 0 1]\n",
            " [0 0 0 0]]\n",
            "Step 3:\n",
            "[[-0.e+00 -1.e+10 -1.e+10 -1.e+10]\n",
            " [-0.e+00 -0.e+00 -1.e+10 -1.e+10]\n",
            " [-0.e+00 -0.e+00 -0.e+00 -1.e+10]\n",
            " [-0.e+00 -0.e+00 -0.e+00 -0.e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect attention\n",
        "\n",
        "Let's unpack the line:\n",
        "\n",
        "`softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v`"
      ],
      "metadata": {
        "id": "C3hZGwhqaVEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's unpack the line: softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
        "\n",
        "n_seq = 6\n",
        "n_q = 4 # should match mask dim\n",
        "d_k = 4\n",
        "n_k = 4\n",
        "d_v = 4\n",
        "\n",
        "causal_mask = (1 - np.tri(d_k, dtype=x.dtype)) * -1e10\n",
        "\n",
        "q = np.random.rand(n_q, d_k) # query matrix [n_q, d_k]\n",
        "k = np.random.rand(n_k, d_k) # key matrix [n_k, d_k]\n",
        "v = np.random.rand(n_k, d_v) # value matrix [n_k, d_v]\n",
        "\n",
        "# Step 1: matmul queries and keys and normalise\n",
        "x1 = q @ k.T / np.sqrt(q.shape[-1])\n",
        "print(f'Step 1:\\n{x1}')\n",
        "# Step 2: add the mask\n",
        "x2 = x1 + causal_mask\n",
        "print(f'Step 2:\\n{x2}')\n",
        "# Step 3: apply softmax to get attention probabilities\n",
        "x3 = softmax(x2)\n",
        "print(f'Step 3:\\n{x3}')\n",
        "# Step 4:\n",
        "x4 = x3 @ v\n",
        "print(f'Step 4:\\n{x4}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C7nITnPWe4b",
        "outputId": "b73fd999-19d4-47e9-b4f8-f69b6be71990"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "[[0.44666548 0.61499874 0.40830254 0.67861022]\n",
            " [0.33612781 0.39504547 0.34757893 0.58094492]\n",
            " [0.44320921 0.66443669 0.50482814 0.76117033]\n",
            " [0.34044291 0.42213636 0.16288176 0.47244036]]\n",
            "Step 2:\n",
            "[[ 4.46665476e-01 -1.00000000e+10 -1.00000000e+10 -1.00000000e+10]\n",
            " [ 3.36127812e-01  3.95045473e-01 -1.00000000e+10 -1.00000000e+10]\n",
            " [ 4.43209208e-01  6.64436690e-01  5.04828141e-01 -1.00000000e+10]\n",
            " [ 3.40442915e-01  4.22136362e-01  1.62881763e-01  4.72440360e-01]]\n",
            "Step 3:\n",
            "[[1.         0.         0.         0.        ]\n",
            " [0.48527484 0.51472516 0.         0.        ]\n",
            " [0.30200858 0.37678808 0.32120334 0.        ]\n",
            " [0.24609095 0.26703898 0.20605436 0.28081571]]\n",
            "Step 4:\n",
            "[[0.68544703 0.89770099 0.92227513 0.99531652]\n",
            " [0.64687409 0.68596656 0.52237634 0.57305228]\n",
            " [0.45449605 0.62522667 0.62950249 0.66102742]\n",
            " [0.60486206 0.58845478 0.49411125 0.75382843]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect gelu\n",
        "x = np.array([[1, 2], [-2, 0.5], [3, 0.1]])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(gelu(x))\n",
        "print(gelu(x).shape)"
      ],
      "metadata": {
        "id": "D6PGX3Rb1IIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a5a0be-dd63-4a4a-b154-d45882b57d55"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.   2. ]\n",
            " [-2.   0.5]\n",
            " [ 3.   0.1]]\n",
            "(3, 2)\n",
            "[[ 0.84119199  1.95459769]\n",
            " [-0.04540231  0.34571401]\n",
            " [ 2.99636261  0.05398275]]\n",
            "(3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect softmax\n",
        "x = np.array([[1, 2], [-2, 0.5], [3, 0.1]])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(softmax(x))\n",
        "print(softmax(x).shape)"
      ],
      "metadata": {
        "id": "kpNFXrFm1IF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb42dc1-edab-4486-8620-2ce09903470d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.   2. ]\n",
            " [-2.   0.5]\n",
            " [ 3.   0.1]]\n",
            "(3, 2)\n",
            "[[0.26894142 0.73105858]\n",
            " [0.07585818 0.92414182]\n",
            " [0.94784644 0.05215356]]\n",
            "(3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Going beyond greedy decoding"
      ],
      "metadata": {
        "id": "g5tE5-y7gwSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(inputs, params, n_head, n_tokens_to_generate, do_sample=False, top_k=0):\n",
        "\n",
        "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
        "\n",
        "        if not do_sample:\n",
        "            # greedy sampling\n",
        "            next_id = np.argmax(logits[-1]) # Note, there is no softmax happening here. Why not?\n",
        "\n",
        "        else:\n",
        "            probs = softmax(logits[-1])\n",
        "\n",
        "            if top_k > 0:\n",
        "                # truncate probs\n",
        "                probs[probs < np.sort(probs)[-top_k]] = 0\n",
        "                probs = probs / np.sum(probs)\n",
        "\n",
        "            next_id = np.random.choice(len(probs), p=probs)\n",
        "\n",
        "        inputs.append(int(next_id))  # append prediction to input\n",
        "\n",
        "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids\n",
        "\n",
        "# let's generate!\n",
        "prompt = \"Not all heroes wear capes.\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "\n",
        "n_tokens_to_generate: int = 40\n",
        "\n",
        "# make sure we are not surpassing the max sequence length of our model\n",
        "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
        "\n",
        "# generate output ids\n",
        "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate, do_sample=True, top_k=10)\n",
        "\n",
        "# decode the ids back into a string\n",
        "output_text = tokenizer.decode(output_ids)\n",
        "\n",
        "print('\\n\\nGenerated sequence:\\n')\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "tj7-iGVm0hA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027ad304-c02f-4858-ac32-ce0df63ced24"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating: 100%|██████████| 40/40 [00:20<00:00,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Generated sequence:\n",
            "\n",
            " Some, like the Iron Maiden, are more popular, and some are not. But they all look the same, and that's what makes this game so exciting: You play the hero and he or\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> What else could we do with the model's next-token scores?\n",
        "\n",
        "> How could we avoid certain tokens tokens from being generated?\n",
        "\n",
        "> How could we force the model to generate certain tokens?\n",
        "\n"
      ],
      "metadata": {
        "id": "RFJjSUq4wvg4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MfPP-pPx0g9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NgXAITs70g5w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}