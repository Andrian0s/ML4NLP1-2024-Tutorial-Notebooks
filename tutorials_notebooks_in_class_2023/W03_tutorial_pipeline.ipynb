{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/tutorials_notebooks_in_class_2023/W03_tutorial_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gzHo80DX3J0"
      },
      "source": [
        "# Introduction to sklearn Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVB8QC6bX3J5"
      },
      "source": [
        "In this tutorial, we will introduce\n",
        "* the Pipeline class of sklearn\n",
        "\n",
        "Based on the notebook of Phillip Ströbel (adjustments from Janis Goldzycher, Andrianos Michail, Patrick Haller)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuLQDR5MX3J6"
      },
      "source": [
        "## Recap\n",
        "\n",
        "We are still working with the text classification data from http://qwone.com/~jason/20Newsgroups/.\n",
        "Cf. notebook from last week for more comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuV1TvUNX3J7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_df(path_to_data, random_state=42):\n",
        "    \"\"\"\n",
        "    Takes the path of a folder containing all the subfolders (which contain the actual documents).\n",
        "    Builds a pandas datafram with document ids, the text and the label.\n",
        "    :param path_to_data: path to top folder as a string\n",
        "    :param random_state: integer, seed for shuffling\n",
        "    :return: pandas dataframe with all th\n",
        "    \"\"\"\n",
        "    doc_list = list()  # doc_list now: [[doc<str>, label<str>], ...]\n",
        "\n",
        "    for category in os.listdir(path_to_data):\n",
        "        for document in os.listdir(os.path.join(path_to_data, category)):\n",
        "            doc = open(os.path.join(path_to_data, category, document), 'r', encoding='latin-1').read().replace('\\n', ' ')\n",
        "            doc_list.append([doc, category])\n",
        "\n",
        "    df = pd.DataFrame(doc_list, columns=['text', 'label'])\n",
        "\n",
        "    return df.sample(frac=1, random_state=random_state) # return and shuffle dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhNP5KxyX3J8"
      },
      "outputs": [],
      "source": [
        "train = create_df('../datasets/20news-bydate/20news-bydate-train')\n",
        "test = create_df('../datasets/20news-bydate/20news-bydate-test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuvWeXL2X3J8"
      },
      "source": [
        "Several ways to inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1hM5sIuX3J-"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIz2S4PX3KA"
      },
      "source": [
        "As usual, we split the labels from the training and the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r6WywPEX3KB"
      },
      "outputs": [],
      "source": [
        "X_train = train.text\n",
        "y_train = train.label\n",
        "X_test = test.text\n",
        "y_test = test.label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmuRdUSSX3KC"
      },
      "source": [
        "Series is just a \"One-dimensional ndarray with axis labels\". Let's see if we got this right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3PoWkuRX3KC"
      },
      "outputs": [],
      "source": [
        "print('Training set shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test set shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqIUdhawuR6d"
      },
      "outputs": [],
      "source": [
        "# randomly sample 5000 documents from the training set from train set\n",
        "X_train = X_train.sample(n=5000, random_state=42)\n",
        "# get same documents from labels\n",
        "y_train = y_train[X_train.index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyUIWl1BX3KD"
      },
      "source": [
        "## Preprocessing and fitting models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUPBuRy5uR6d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_T0KafxX3KD"
      },
      "outputs": [],
      "source": [
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)  # num_docs x num_words\n",
        "X_test_counts = count_vect.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJXMA0q_X3KI"
      },
      "outputs": [],
      "source": [
        "tfidf_tranformer = TfidfTransformer(smooth_idf=True).fit(X_train_counts)\n",
        "X_train_tfidf = tfidf_tranformer.transform(X_train_counts)\n",
        "X_test_tfidf = tfidf_tranformer.transform(X_test_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhdcRdMOX3KJ"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI33TO6QX3KM"
      },
      "outputs": [],
      "source": [
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf, y_train)\n",
        "scores = cross_val_score(svc, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYBL2HgbuR6f"
      },
      "outputs": [],
      "source": [
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1_rQidWX3KN"
      },
      "source": [
        "We can also calculate precision, recall, and f1 relatively easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H04oOtkMX3KN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(max_iter=50)\n",
        "sgd_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_train_predictions = cross_val_predict(sgd_clf, X_train_tfidf, y_train, cv=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jliIOiCkuR6f"
      },
      "outputs": [],
      "source": [
        "# maybe add confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br-EQ2AoX3KO"
      },
      "source": [
        "## Shortcuts in sklearn: Pipelines\n",
        "Sklearn allows us to build convenient `Pipelines`, which facilitate the management of our data and the training of our models enourmously. Consider for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbTKdcoeX3KO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqnAPtIYuR6g"
      },
      "outputs": [],
      "source": [
        "# Define a pipeline: first vectorize, then tfidf, then classify\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LogisticRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRzTnitXuR6g"
      },
      "outputs": [],
      "source": [
        "# Q: What types of ngrams are used here?\n",
        "# Q: What type of regularization did we used here? How can we change it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx1_FDKcX3KO"
      },
      "source": [
        "We could even replace the two first lines of the pipeline by using `TfidfVectorizer`, which first fits and transforms the input the same way as the `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCysE7tLX3KO"
      },
      "outputs": [],
      "source": [
        "text_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmX9mh6eX3KP"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(text_clf, X_train, y_train, scoring='accuracy', cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBKGQzJLX3KP"
      },
      "outputs": [],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWKm1CyaX3KP"
      },
      "source": [
        "## Model selection - find your best model\n",
        "For every model you would like to train, there is a plethora of parameters you could set. How to find the best model? Again, sklearn has a solution: `GridSearchCV`. With grid search cross validation, you can set your hyperparameter space and train different models with all the parameter combinations. Keep in mind that depending on how many folds you train, the whole training procedure takes significantly longer. But let's set up grid search cross validation. We set up a new pipeline for a SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuyNDDbjX3KP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "text_svc = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('svc', LinearSVC())\n",
        "])\n",
        "\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "             'svc__loss': ['hinge', 'squared_hinge'],\n",
        "             'svc__multi_class': ['ovr', 'crammer_singer']}\n",
        "\n",
        "# Q: With how many combinations of parameters will we end up?\n",
        "\n",
        "gs_svc = GridSearchCV(text_svc, param_grid, cv=5, verbose=1)\n",
        "gs_svc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyKZRLITX3KP"
      },
      "outputs": [],
      "source": [
        "svc_df = pd.DataFrame.from_dict(gs_svc.cv_results_)\n",
        "svc_df.sort_values(by=[\"rank_test_score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyPQWIOzX3KQ"
      },
      "outputs": [],
      "source": [
        "best_model = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('svc', LinearSVC(loss='hinge', multi_class='crammer_singer'))\n",
        "])\n",
        "\n",
        "best_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCF056HrX3KQ"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "for index, prediction in enumerate(best_model.predict(X_test)):\n",
        "    if prediction == y_test[index]:\n",
        "        correct +=1\n",
        "\n",
        "print('Accuracy: ', correct/y_test.shape[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MZ6F8ZM1X3KJ",
        "vOpydQgDX3KK"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}