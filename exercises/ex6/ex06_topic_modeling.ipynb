{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/exercises/ex6/ex06_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hENmFU8FmZbl"
      },
      "source": [
        "!pip install -qU contextualized-topic-models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Instructions\n",
        "\n",
        "1. Perform Topic Modeling using LDA and CTM on the three time frames: before 1990, 1990-2009 and 2010 onwards.\n",
        "2. Experiment with a) different preprocessing functions and b) varying number of topics.\n",
        "3. Annotate the topics.\n",
        "4. Answer the questions marked with üìù‚ùì in your lab report at the end of this notebook  "
      ],
      "metadata": {
        "id": "-QgZ2SlBbtEX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAwbguDJlC1m"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci16zWWnlC1n"
      },
      "source": [
        "import re\n",
        "import urllib\n",
        "import gzip\n",
        "import io\n",
        "import csv\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset"
      ],
      "metadata": {
        "id": "aydW6ZdJc-kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_before_1990 = 'https://drive.google.com/file/d/1o_IeJCqvDLH5xgjYYuEHoPuPjF7SYvwR/view?usp=drive_link'\n",
        "url_from_1990_to_2009 = 'https://drive.google.com/file/d/1Q31iYPxlcsvB0nwGter3RDfbhVRtV2yI/view?usp=drive_link'\n",
        "url_from_2010 = 'https://drive.google.com/file/d/1s7pLqaiMVxM0M4WBKgZpBxNDFKXeQ47x/view?usp=drive_link'"
      ],
      "metadata": {
        "id": "A2x0MpAcKYRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to download data given a google drive url - Returns a list\n",
        "import requests\n",
        "\n",
        "def download_text_file_from_drive(drive_url):\n",
        "    try:\n",
        "        file_id = drive_url.split('/d/')[1].split('/')[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Google Drive URL format. Ensure it includes '/d/<file_id>/'.\")\n",
        "\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}&export=download\"\n",
        "\n",
        "    response = requests.get(download_url)\n",
        "    if response.status_code != 200:\n",
        "        raise RuntimeError(f\"Failed to download file. HTTP Status Code: {response.status_code}\")\n",
        "\n",
        "    content = response.text\n",
        "    titles_year = content.splitlines()\n",
        "    titles = [x.split(',')[0] for x in titles_year]\n",
        "    return titles"
      ],
      "metadata": {
        "id": "i_vvNNMmMiok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles_before_1990 = download_text_file_from_drive(url_before_1990)\n",
        "titles_from_1990_to_2009 = download_text_file_from_drive(url_from_1990_to_2009)\n",
        "titles_from_2010 = download_text_file_from_drive(url_from_2010)\n",
        "\n",
        "# Check the length of downloaded data\n",
        "print(len(titles_before_1990))\n",
        "print(len(titles_from_1990_to_2009))\n",
        "print(len(titles_from_2010))\n",
        "\n",
        "# Check the first element of each list\n",
        "# Elements in the list are of the format - paper_title, year\n",
        "print(titles_before_1990[0])\n",
        "print(titles_from_1990_to_2009[0])\n",
        "print(titles_from_2010[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF0bl_KYMr55",
        "outputId": "d290e9cd-b4f5-415f-a92f-2d954b8b1b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40000\n",
            "243581\n",
            "582378\n",
            "An Introduction to Mathematical Taxonomy\n",
            "The Future of Classic Data Administration: Objects + Databases + CASE\n",
            "E. W. Dijkstra Archive: The manuscripts of Edsger W. Dijkstra 1930-2002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Functions"
      ],
      "metadata": {
        "id": "i0hcVyZIdgAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Optionally, you can write the preprocessing functions for LDA here or use inbuilt sklearn functionalities for preprocessing while performing LDA*\n",
        "\n",
        "*For CTMs, it is recommended that you preprocess the dataset only for creating Bag of Words, while the embeddings are generated without doing any preprocessing. This will ensure that better quality embeddings are generated as more context is present, without the vocabulary size becoming huge. You can refer to authors' proposed preprocessing implementation [here](https://github.com/MilaNLProc/contextualized-topic-models?tab=readme-ov-file#preprocessing)*"
      ],
      "metadata": {
        "id": "zfyabgzoigDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess1():\n",
        "    return"
      ],
      "metadata": {
        "id": "IfEugcPCdt8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess2():\n",
        "    return"
      ],
      "metadata": {
        "id": "oreau7hWdyMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDhEoD2Tsh3z"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ86sQwo124U"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "num_lda_topics = 5 # min number of topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc09sFDe-5kn"
      },
      "source": [
        "### Before the 1990s:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rwq6kF7vqUj"
      },
      "source": [
        "# Read data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vmu77l62DCp"
      },
      "source": [
        "# Preprocess 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mulG5Sg2NBZ"
      },
      "source": [
        "# Preprocess 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgxgCM_01mx"
      },
      "source": [
        "# Perform LDA with num_lda_topics = 5 for Preprocess 1 - Annotate the topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_927CQlk4C27"
      },
      "source": [
        "# Perform LDA with num_lda_topics = 5 for Preprocess 2 - Annotate the topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-8NinuI4kMG"
      },
      "source": [
        "# Perform LDA with num_lda_topics > 5 for Preprocess 1 - Annotate the topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform LDA with num_lda_topics > 5 for Preprocess 2 - Annotate the topics"
      ],
      "metadata": {
        "id": "PZosO7GwkWli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqioi0rG_Dz9"
      },
      "source": [
        "### From 1990 to 2009:\n",
        "\n",
        "*Add your code for topic modelling the period from 1990 to 2009 here - similar to what you did for before 1990s*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip_z250Z_Mnz"
      },
      "source": [
        "### From 2010 onwards:\n",
        "\n",
        "*Add your code for topic modelling the period from 2010 onwards here - similar to what you did for before 1990s*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù‚ùì For each period, assign a name to each generated topic based on the topic‚Äôs top words. List all topic names in your report. If a topic is incoherent to the degree that no common theme is detectable, you can just mark it as incoherent (i.e., no need to name a topic that does not exist).\n",
        "\n",
        "üìù‚ùì Do the topics make sense to you? Are they coherent? Do you observe trends across different time periods? Discuss in 4-6 sentences.\n"
      ],
      "metadata": {
        "id": "lkTgLxUCnBZO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pStFBikpsRTz"
      },
      "source": [
        "## Combined Topic Models\n",
        "\n",
        "Method developed by [Bianchi et al. 2021](https://aclanthology.org/2021.acl-short.96/).\n",
        "\n",
        "[A 6min presentation of the paper by one of the authors.](https://underline.io/lecture/25716-pre-training-is-a-hot-topic-contextualized-document-embeddings-improve-topic-coherence)\n",
        "\n",
        "[Medium Blog](https://towardsdatascience.com/contextualized-topic-modeling-with-python-eacl2021-eacf6dfa576)\n",
        "\n",
        "Code: [https://github.com/MilaNLProc/contextualized-topic-models](https://github.com/MilaNLProc/contextualized-topic-models)\n",
        "\n",
        "Tutorial: [https://colab.research.google.com/drive/1fXJjr_rwqvpp1IdNQ4dxqN4Dp88cxO97?usp=sharing](https://colab.research.google.com/drive/1fXJjr_rwqvpp1IdNQ4dxqN4Dp88cxO97?usp=sharing)\n",
        "\n",
        "Again, perform topic modelling for the three time periods - this time using the combined topic models (CTMs).\n",
        "\n",
        "You can use and adapt the code from the tutorial linked above.\n",
        "\n",
        "Use the available GPU for faster running times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNsJrV2DJ8GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419af43a-2ea0-41b2-f077-8d6babb5cf15"
      },
      "source": [
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***Important - Executing the import below (WhiteSpacePreprocessing) will produce an error on the first run. Executing it again mitigates the error. This is probably due to some caching issues with contextualized_topic_models package***"
      ],
      "metadata": {
        "id": "b4qfsujFlSnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing"
      ],
      "metadata": {
        "id": "gF3d_qKZlRVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing"
      ],
      "metadata": {
        "id": "NUC536wylZ9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_ctm_topics = 5  # min number of topics\n",
        "MODEL_NAME = \"sentence-transformers/paraphrase-mpnet-base-v2\" # Model to use for CTM"
      ],
      "metadata": {
        "id": "0fUGqa6wb1Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSv9Ch46LFxp"
      },
      "source": [
        "### Before the 1990s:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess 1"
      ],
      "metadata": {
        "id": "eaKUWA7Pl6zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess 2"
      ],
      "metadata": {
        "id": "GP2Cvhazl9WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform CTM with num_ctm_topics = 5 for Preprocess 1 - Annotate the topics"
      ],
      "metadata": {
        "id": "iWGpouRol_Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform CTM with num_ctm_topics = 5 for Preprocess 2 - Annotate the topics"
      ],
      "metadata": {
        "id": "ii_3ErGUmBm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform CTM with num_ctm_topics > 5 for Preprocess 1 - Annotate the topics"
      ],
      "metadata": {
        "id": "JFoLQsj0mHGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform CTM with num_ctm_topics > 5 for Preprocess 2 - Annotate the topics"
      ],
      "metadata": {
        "id": "uaIg3PRnmLWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNRaJJEMiVQb"
      },
      "source": [
        "### From 1990 to 2009"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your code for topic modelling the period from 1990 to 2009 here - similar to what you did for before 1990s"
      ],
      "metadata": {
        "id": "U1sjHRS2mbUW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfgbLy0KiZ5b"
      },
      "source": [
        "### From 2010 onwards"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add your code for topic modelling the period from 2010 onwards - similar to what you did for before 1990s"
      ],
      "metadata": {
        "id": "A_3zKk3omc1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù‚ùì Again: Assign a name to each topic based on the topic‚Äôs top words (for each period). List all topic names in your report.\n",
        "\n",
        "üìù‚ùì Bianchi et al. 2021 claim that their approach produces more coherent topics than previous methods. Let‚Äôs test this claim by comparing the coherence of the topics produced by CTM with the topics produced by LDA. Describe your observations in 3-4 sentences.\n",
        "\n",
        "üìù‚ùì Do the two models generate similar topics? Can you discover the same temporal trends (if there are any)? Discuss in 5-6 sentences.\n",
        "\n",
        "üìù‚ùì Can you suggest an alternate model apart from paraphrase-mpnet-base-v2? What could be some of the possible advantages and disadvantages of using an alternate model? Hint: Look at some of the models [here](https://huggingface.co/spaces/mteb/leaderboard). Note: You do not need to execute the code for an alternate model."
      ],
      "metadata": {
        "id": "i0ImWrmYnif0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Report"
      ],
      "metadata": {
        "id": "fof4ZCvomnCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mvxxmg77mpsa"
      }
    }
  ]
}