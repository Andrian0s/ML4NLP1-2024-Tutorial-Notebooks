{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/exercises/ex5/ex5_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: LLM Prompting and Prompt Engineering Part 2\n",
        "\n",
        "In part 2, we experiment with prompting instruction-tuned Large Language Models (LLMs), and evaluate their performance on a linguistic annotation task involving structured outputs.\n",
        "\n",
        "The goal of this assignment is to gain some experience working with instruction-tuned LLMs. To this end, you will learn how to\n",
        "\n",
        "- query an instruction-tuned LLM with default chat templates (`Llama-3.2-3B-Instruct`)\n",
        "- parse LLM outputs for structured responses using JSON and `Pydantic`\n",
        "- implement error handling for edge cases where the model fails to output the expected data format.\n",
        "\n",
        "The task we use for this purpose is a simple Tokenization and Part-of-Speech tagging task using data taken from Universal Dependencies.\n",
        "\n",
        "To facilitate working with LLMs, we will again use the Unsloth library. Note that Unsloth provides both freeware and closed-source proprietary software. For our purposes, the freeware is sufficient! For more information on Unsloth, see the docs here.\n",
        "\n",
        "This notebook is adapted from [this example](https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing) by Unsloth.\n",
        "\n",
        "\n",
        "### NOTE: Expected execution times\n",
        "We have provided expected execution times throughout the notebook as a guide. These are intended to be approximate, but should give you some idea for what to expect. If your runtimes far exceed these expected execution times, you may want to consider modifying your approach. These are denoted with ⌛ .\n",
        "\n",
        "### NOTE: GPU Usage\n",
        "It is expected that you load the model onto a GPU for inference. For other parts of the code, such as data preparation, a GPU is not necessary. To avoid waiting for resources unnecessarily, we recommend doing as much as you can on a CPU instance and change the runtime type as necessary. We've highlight the cells that need a GPU with ⚡"
      ],
      "metadata": {
        "id": "kekuLuANuoMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Installing dependencies"
      ],
      "metadata": {
        "id": "Ue7JmxZoXtV2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install levenshtein\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check unsloth version\n",
        "expected_version = '2024.10.2'\n",
        "unsloth_version = !pip list | grep -P 'unsloth\\s+' | grep -Po '\\S+$'\n",
        "if unsloth_version[0] != expected_version:\n",
        "    print(f\"Warning! Found Unsloth version {unsloth_version[0]} but expected {expected_version}.\")\n",
        "\n",
        "# check python version\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# check gpu info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# check RAM info\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "c_LEUjzo4sfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Model Loading"
      ],
      "metadata": {
        "id": "-VDpOQwNwxi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Note, here we specify the instruction-tuned version of Llama-3.2-3B\n",
        "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        "    )\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Data Loading and Preparation"
      ],
      "metadata": {
        "id": "7owhdpPE7i8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "seed = 42\n",
        "\n",
        "random.seed(seed)\n",
        "\n",
        "dataset_url = \"https://raw.githubusercontent.com/tannonk/prompting_exercise/refs/heads/main/data/en_ewt-ud-dev-pos.json\"\n",
        "df = pd.read_json(dataset_url, lines=True)\n",
        "\n",
        "# For each input sentence, we'll build the target as a list of dictionaries containing keys for the token and pos tag. This is what we want our LLM annotator to predict.\n",
        "df['target'] = df.apply(lambda x: [{'token': token, 'pos': pos} for token, pos in zip(x['tokens'].split(), x['upos'].split())], axis=1)\n",
        "\n",
        "# We'll sample 100 items for testing purposes\n",
        "test_data = df.sample(n=100, random_state=seed)\n",
        "train_data = df.drop(test_data.index)\n",
        "\n",
        "print(f\"Train data: {len(train_data)}\")\n",
        "print(f\"Test data: {len(test_data)}\")\n",
        "\n",
        "test_data.head()\n"
      ],
      "metadata": {
        "id": "IcrSdhyr7ecF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Inspect and describe the data\n",
        "\n",
        "📝❓ What are the fields and their corresponding values in the dataframe?\n",
        "\n",
        "📝❓ What is the difference between `upos` and `xpos`?\n",
        "\n",
        "📝❓ What is the distribution of `upos` labels in the `test_data`?"
      ],
      "metadata": {
        "id": "Zc4OwDRnwIpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Define the basic `PromptTemplate`\n",
        "\n",
        "Note, you can reuse the solution from part 1 of this exercise here."
      ],
      "metadata": {
        "id": "WSqsJYxpwqD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "bx9wBnZx7eZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "## 3.2 ChatTemplates\n",
        "\n",
        "Instruction-tuned models are typically finetuned using a predefined `ChatTemplate`.\n",
        "This means that when using them for inference, it is important that we use the correct `ChatTemplate` in order to avoid \"confusing\" the model.\n",
        "\n",
        "You can find more information about model `ChatTemplates` for Huggingface models [here](https://huggingface.co/docs/transformers/en/chat_templating).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# load the chat_template from unsloth (note, the logic is similar when using native Huggingface, but here we're using Unsloth!)\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\", # for Llama-3.1 and Llama-3.2 models\n",
        ")\n",
        "\n",
        "# Inspect the template (note, it looks more complicated than it is!)\n",
        "print(tokenizer.chat_template)\n"
      ],
      "metadata": {
        "id": "wvUc0oCp-oMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Prepare your inputs using the `ChatTemplate` for the model.\n",
        "\n",
        "Note, you should be able to drop your custom `PromptTemplate` string into the model's default `ChatTemplate`.\n"
      ],
      "metadata": {
        "id": "0PE4qKNHbZU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "mJYe8Vfx-oKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Inference Pipeline"
      ],
      "metadata": {
        "id": "xobpn1UhxZZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Define a function to run inference efficiently with an LLM\n",
        "\n",
        "Note, you can use the same inference function from part 1 of this exercise here!"
      ],
      "metadata": {
        "id": "CeMEUBEebsFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up our inference pipeline for generation\n",
        "\n",
        "# We'll set some default generation args that we'll pass to our inference function\n",
        "# Following best practices, we'll use Pydantic class which helps with validation.\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Generation_Args(BaseModel):\n",
        "    max_new_tokens: int\n",
        "    temperature: float\n",
        "    top_k: int\n",
        "    top_p: float\n",
        "    repetition_penalty: float\n",
        "    do_sample: bool\n",
        "    min_p: float\n",
        "    num_return_sequences: int\n",
        "\n",
        "# Here are some default generation args\n",
        "generation_args = Generation_Args(\n",
        "    max_new_tokens = 1024, # note, for this task, we're setting the max_new_tokens to be more appropriate\n",
        "    temperature = 1.0,\n",
        "    top_k = 0,\n",
        "    top_p = 1.0,\n",
        "    repetition_penalty = 1.0,\n",
        "    do_sample = True,\n",
        "    use_cache = True,\n",
        "    min_p = 0.1,\n",
        "    num_return_sequences = 1\n",
        ")\n",
        "\n",
        "\n",
        "def run_batched_inference(prompts, model, tokenizer, batch_size=10, generation_args=generation_args):\n",
        "    \"\"\"\n",
        "    Runs batched inference on a list of prompts using a given model and tokenizer.\n",
        "\n",
        "    Set the batch_size to control the number of prompts processed in each batch.\n",
        "    Depending on the length of your prompts and model size the batch size may need to be adjusted.\n",
        "\n",
        "    Args:\n",
        "        prompts (list[str]): List of prompts that are passed to the model\n",
        "        model (): The model used for generation\n",
        "        tokenizer (): The tokenizer used for encoding and decoding the prompts\n",
        "        batch_size (int): Number of prompts to run batched inference on.\n",
        "\n",
        "    Returns:\n",
        "        List[str] containing generated outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    # TODO: implement the logic for efficient inference with LLM\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "kkICV3hAAUQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1) Run Inference\n",
        "\n",
        "### TODO: run inference!\n",
        "\n",
        "⌛ 10-20 mins\n",
        "\n",
        "⚡ GPU"
      ],
      "metadata": {
        "id": "4BcvKAh58g0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: inference"
      ],
      "metadata": {
        "id": "5cHPJJyJ825R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Structured output validation\n",
        "\n",
        "LLMs output text. But in practice, we often want structured data that we can process further with other automatic processes.\n",
        "\n",
        "For this purpose, JSON is a good target data structure.\n"
      ],
      "metadata": {
        "id": "2ILZF5EYISwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Define a processing pipeline that extracts and validates the JSON response from the LLM.\n",
        "\n",
        "Hint: For this you should use a combination of [`Regex`](https://www.w3schools.com/python/python_regex.asp) and [`Pydantic`](https://docs.pydantic.dev/latest/).\n",
        "\n",
        "The output should be a valid json object with the following structure:\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\"token\": \"there\", \"pos\": \"DET\"}, # each dict contains a token and its corresponding POS-Tag.\n",
        "    {\"token\": \"is\", \"pos\": \"VERB\"},\n",
        "    {\"token\": \"no\", \"pos\": \"ADJ\"},\n",
        "    {\"token\": \"companion\", \"pos\": \"NOUN\"},\n",
        "    {\"token\": \"quite\", \"pos\": \"ADV\"},\n",
        "    {\"token\": \"so\", \"pos\": \"ADV\"},\n",
        "    {\"token\": \"devoted\", \"pos\": \"ADV\"},\n",
        "    {\"token\": \"so\", \"pos\": \"ADV\"},\n",
        "    ...\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "ixlnaY4Jbw8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "8Skma6ohHlkN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) *Evaluation*"
      ],
      "metadata": {
        "id": "UbijgXU4jaVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is some boilerplate evaluation code. You should not need to make any changes here.\n",
        "\n",
        "import Levenshtein\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "def evaluate_instance(target: List[Dict], prediction: List[Dict]):\n",
        "    \"\"\"\n",
        "    Evaluates the accuracy of tokenization and part-of-speech (POS) tagging between a target and a predicted sequence.\n",
        "\n",
        "    Args:\n",
        "        target (List[Dict]): A list of dictionaries representing the target tokens and POS tags.\n",
        "        prediction (List[Dict]): A list of dictionaries representing the predicted tokens and POS tags.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the token-level accuracy ('Token Acc') and POS accuracy ('POS Acc').\n",
        "    \"\"\"\n",
        "\n",
        "    # If there is no prediction, return zero accuracies\n",
        "    if prediction is None:\n",
        "        return {'Token Acc': 0, 'POS Acc': 0}\n",
        "\n",
        "    # Extract tokens and POS tags from the target and prediction lists\n",
        "    target_tokens = [item['token'] for item in target]\n",
        "    target_pos = [item['pos'] for item in target]\n",
        "    pred_tokens = [item['token'] for item in prediction]\n",
        "    pred_pos = [item['pos'] for item in prediction]\n",
        "\n",
        "    # Get alignment operations between the target and predicted tokens using Levenshtein.opcodes()\n",
        "    opcodes = Levenshtein.opcodes(target_tokens, pred_tokens)\n",
        "\n",
        "    # Initialize aligned lists to store tokens and POS tags after alignment\n",
        "    aligned_target_tokens = []\n",
        "    aligned_target_pos = []\n",
        "    aligned_pred_tokens = []\n",
        "    aligned_pred_pos = []\n",
        "\n",
        "    # Iterate over each operation in the alignment\n",
        "    for tag, i1, i2, j1, j2 in opcodes:\n",
        "        # \"equal\" means the tokens in this range are identical in both sequences\n",
        "        if tag == 'equal':\n",
        "            aligned_target_tokens.extend(target_tokens[i1:i2])\n",
        "            aligned_target_pos.extend(target_pos[i1:i2])\n",
        "            aligned_pred_tokens.extend(pred_tokens[j1:j2])\n",
        "            aligned_pred_pos.extend(pred_pos[j1:j2])\n",
        "        # \"replace\" means tokens in this range are different between the target and prediction\n",
        "        elif tag == 'replace':\n",
        "            aligned_target_tokens.extend(target_tokens[i1:i2])\n",
        "            aligned_target_pos.extend(target_pos[i1:i2])\n",
        "            aligned_pred_tokens.extend(pred_tokens[j1:j2])\n",
        "            aligned_pred_pos.extend(pred_pos[j1:j2])\n",
        "        # \"insert\" means tokens were added in the prediction that are not in the target\n",
        "        elif tag == 'insert':\n",
        "            aligned_target_tokens.extend(['<MISSING>'] * (j2 - j1))  # Add placeholders for missing target tokens\n",
        "            aligned_target_pos.extend(['<MISSING>'] * (j2 - j1))      # Add placeholders for missing target POS tags\n",
        "            aligned_pred_tokens.extend(pred_tokens[j1:j2])\n",
        "            aligned_pred_pos.extend(pred_pos[j1:j2])\n",
        "        # \"delete\" means tokens are present in the target but missing in the prediction\n",
        "        elif tag == 'delete':\n",
        "            aligned_target_tokens.extend(target_tokens[i1:i2])\n",
        "            aligned_target_pos.extend(target_pos[i1:i2])\n",
        "            aligned_pred_tokens.extend(['<MISSING>'] * (i2 - i1))    # Add placeholders for missing predicted tokens\n",
        "            aligned_pred_pos.extend(['<MISSING>'] * (i2 - i1))       # Add placeholders for missing predicted POS tags\n",
        "\n",
        "    # Calculate token-level accuracy\n",
        "    # We only consider positions where both target and prediction have valid tokens (i.e., not '<MISSING>')\n",
        "    correct_tokens = [\n",
        "        1 if tgt == pred else 0\n",
        "        for tgt, pred in zip(aligned_target_tokens, aligned_pred_tokens)\n",
        "        if tgt != '<MISSING>' and pred != '<MISSING>'\n",
        "    ]\n",
        "    token_accuracy = np.mean(correct_tokens) if correct_tokens else 0\n",
        "\n",
        "    # Calculate POS accuracy\n",
        "    # Only consider positions where tokens match and are not '<MISSING>'\n",
        "    correct_pos = [\n",
        "        1 if tgt_pos == pred_pos else 0\n",
        "        for tgt_tok, pred_tok, tgt_pos, pred_pos in zip(aligned_target_tokens, aligned_pred_tokens, aligned_target_pos, aligned_pred_pos)\n",
        "        if tgt_tok == pred_tok and tgt_tok != '<MISSING>'\n",
        "    ]\n",
        "    pos_accuracy = np.mean(correct_pos) if correct_pos else 0\n",
        "\n",
        "    return {'Token Acc': token_accuracy, 'POS Acc': pos_accuracy}\n",
        "\n",
        "def get_results(test_data: pd.DataFrame, processed_outputs: List[List[Dict]]):\n",
        "    \"\"\"\n",
        "    Returns a summary dataframe by taking the average of the all results for tokenization and pos-tagging.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i in range(len(processed_outputs)):\n",
        "        results.append(evaluate_instance(test_data.iloc[i]['target'], processed_outputs[i]))\n",
        "\n",
        "    results = pd.DataFrame(results).mean()\n",
        "    return results"
      ],
      "metadata": {
        "id": "QkD2UbYZ7RDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the results, you should be able to pass your test_data DataFrame and the processed_outputs from above...\n",
        "get_results(test_data, processed_outputs)"
      ],
      "metadata": {
        "id": "g54ebTc-1VYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Manipulating the system prompt\n",
        "\n",
        "The system prompt is part of the `ChatTemplate` that can help to steer the model.\n"
      ],
      "metadata": {
        "id": "MhjNb5QClvTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Customise the system prompt for the intended task and re-run inference\n",
        "\n",
        "Note, this is an experiment. You should try a few different system prompts and report the resulting performance in your report.\n",
        "\n",
        "\n",
        "📝❓ What was the best system prompt you considered?\n",
        "\n",
        "📝❓ Were you able to improve the performance by manipulating the system prompt? Please discuss.\n",
        "\n",
        "⌛ 10-20 mins (per experiment run)\n",
        "\n",
        "⚡ GPU"
      ],
      "metadata": {
        "id": "XZ4NOohvb6Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: inference"
      ],
      "metadata": {
        "id": "yQRBsLTIhoGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 8) Lab report\n",
        "\n",
        "📝❓ Write your lab report here addressing all questions in the notebook"
      ],
      "metadata": {
        "id": "xKiPb2fvv8d_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nVpkidJXdO5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}